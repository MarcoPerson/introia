# üî¢ Module 2 : Vectorisation - Transformer les Mots en Nombres

> **"Comment expliquer √† un ordinateur que 'roi' et 'reine' sont similaires ?"**
> 
> Bienvenue dans l'univers fascinant de la vectorisation ! Ici, nous transformons des mots en coordonn√©es math√©matiques pour que les machines comprennent enfin le **sens** derri√®re les mots.

---

## üìπ **Vid√©o d'Introduction** *(5 minutes)*

üé¨ **[Lien vers la vid√©o : "De Shakespeare aux Math√©matiques"]**

**R√©sum√© de la vid√©o :**
> Dans cette vid√©o, vous d√©couvrirez pourquoi un ordinateur ne peut pas naturellement comprendre que "excellent" et "fantastique" veulent dire la m√™me chose. Je vous montrerai comment les techniques de vectorisation permettent de cr√©er un "GPS du langage" o√π chaque mot a ses coordonn√©es dans un espace de sens. Vous verrez des d√©monstrations live de word embeddings qui r√©solvent des analogies comme "roi - homme + femme = reine" et comprendrez pourquoi Google Translate s'am√©liore sans cesse. √Ä la fin de ce module, vous aurez cr√©√© votre propre d√©tecteur de plagiat plus malin que celui de votre universit√© !

**Points cl√©s abord√©s :**
- ‚ú® D√©monstration visuelle : mots dans l'espace 3D
- üßÆ Du comptage simple aux embeddings sophistiqu√©s
- üéØ Applications concr√®tes : d√©tection de similarit√©, recommendation
- üöÄ Teasing du projet final : d√©tecteur de plagiat

---

## üéØ **Objectifs d'Apprentissage**

√Ä la fin de ce module, vous serez capable de :

- [ ] **Expliquer** pourquoi les machines ont besoin de transformer les mots en nombres
- [ ] **Impl√©menter** les techniques Bag of Words et TF-IDF from scratch
- [ ] **Utiliser** des word embeddings pr√©-entra√Æn√©s pour des t√¢ches pratiques
- [ ] **Calculer** la similarit√© entre textes avec diff√©rentes m√©thodes
- [ ] **Cr√©er** un syst√®me de d√©tection de similarit√© fonctionnel
- [ ] **Analyser** les avantages/inconv√©nients de chaque approche

---

## üìö **Plan du Module**

| Section | Contenu | Dur√©e | Difficult√© |
|---------|---------|-------|------------|
| **1** | Le Probl√®me de Repr√©sentation | 45 min | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ |
| **2** | Bag of Words (BoW) | 60 min | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ |
| **3** | TF-IDF | 75 min | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ |
| **4** | Word Embeddings | 90 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ |
| **5** | Calcul de Similarit√© | 60 min | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ |
| **Exercices** | 4 exercices pratiques | 120 min | Variable |
| **Projet** | D√©tecteur de similarit√© | 90 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ |

**Total estim√© :** 4-5 heures

---

## üìñ **Section 1 : Le Probl√®me de Repr√©sentation**

### ü§î **Pourquoi les Mots ne Sont pas des Nombres ?**

Imaginez que vous essayez d'expliquer √† un alien (votre ordinateur) les relations entre les mots humains. Comment lui faire comprendre que :

- "Chat" et "F√©lin" sont proches
- "Roi" et "Reine" partagent un concept de royaut√©
- "Courir" et "Sprint" sont des variations d'intensit√©

**Le d√©fi fondamental :** Les ordinateurs ne manipulent que des nombres (0 et 1), mais le langage humain est fait de symboles abstraits charg√©s de sens.

### üó∫Ô∏è **L'Espace Vectoriel du Langage**

**Analogie GPS :** Tout comme chaque lieu sur Terre a des coordonn√©es (latitude, longitude), nous pouvons donner des "coordonn√©es de sens" √† chaque mot.

```python
# Exemple conceptuel
mots_coordonnees = {
    "roi": [0.8, 0.2, 0.9],      # [pouvoir, genre_masculin, noblesse]
    "reine": [0.8, 0.8, 0.9],    # [pouvoir, genre_feminin, noblesse]
    "chat": [0.1, 0.5, 0.2],     # [pouvoir, genre_neutre, noblesse]
}
```

### üéØ **Applications Concr√®tes**

**O√π utilisez-vous d√©j√† la vectorisation sans le savoir ?**

- üîç **Moteurs de recherche** : Google trouve des documents similaires √† votre requ√™te
- üéµ **Spotify** : Recommandations bas√©es sur la similarit√© des descriptions musicales
- üõí **E-commerce** : "Les clients qui ont aim√© X ont aussi aim√© Y"
- üåê **Traduction** : Aligner les concepts entre langues diff√©rentes

### üí° **Les D√©fis √† Relever**

1. **Synonymes** : "Voiture" et "automobile" doivent √™tre proches
2. **Polys√©mie** : "Avocat" (fruit) vs "avocat" (m√©tier)
3. **Contexte** : "Pomme" dans "pomme de terre" vs "pomme rouge"
4. **N√©gation** : "Pas bon" ‚â† "bon"

---

## üìñ **Section 2 : Bag of Words (BoW) - L'Approche Naive**

### üéí **Le Principe du Sac de Mots**

**M√©taphore :** Imaginez que vous videz un livre dans un sac et que vous comptez chaque mot, **en ignorant l'ordre**.

```python
# Exemple simple
phrases = [
    "Le chat mange",
    "Le chien mange aussi",
    "Chat et chien sont amis"
]

# Vocabulaire global
vocabulaire = ["le", "chat", "mange", "chien", "aussi", "et", "sont", "amis"]

# Repr√©sentation BoW
bow_representations = [
    [1, 1, 1, 0, 0, 0, 0, 0],  # "Le chat mange"
    [1, 0, 1, 1, 1, 0, 0, 0],  # "Le chien mange aussi"
    [0, 1, 0, 1, 0, 1, 1, 1]   # "Chat et chien sont amis"
]
```

### üõ†Ô∏è **Impl√©mentation Manuelle**

```python
def create_bow_manual(documents):
    """
    Cr√©e une repr√©sentation Bag of Words from scratch
    """
    # √âtape 1: Construire le vocabulaire
    vocabulaire = set()
    for doc in documents:
        vocabulaire.update(doc.lower().split())
    
    vocab_list = sorted(list(vocabulaire))
    
    # √âtape 2: Vectoriser chaque document
    bow_matrix = []
    for doc in documents:
        words = doc.lower().split()
        vector = [words.count(word) for word in vocab_list]
        bow_matrix.append(vector)
    
    return bow_matrix, vocab_list

# Test
documents = [
    "Python est g√©nial",
    "J'adore programmer en Python",
    "Le machine learning avec Python"
]

bow_matrix, vocabulaire = create_bow_manual(documents)
print("Vocabulaire:", vocabulaire)
print("Matrice BoW:", bow_matrix)
```

### ‚öñÔ∏è **Avantages et Limitations**

| ‚úÖ **Avantages** | ‚ùå **Limitations** |
|------------------|-------------------|
| Simple √† comprendre | Perte de l'ordre des mots |
| Rapide √† calculer | Pas de contexte s√©mantique |
| Fonctionne sur tous les langages | Probl√®me de dimensionnalit√© |
| Base solide pour d'autres techniques | Sensible aux mots fr√©quents |

### üß™ **Exemple Pratique : Classification de Textes**

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# Donn√©es d'exemple
textes = [
    "Ce film est fantastique",
    "J'ai ador√© ce film",
    "Film d√©cevant et ennuyeux",
    "Tr√®s mauvais film",
    "Excellent divertissement",
    "Perte de temps totale"
]
labels = ["positif", "positif", "n√©gatif", "n√©gatif", "positif", "n√©gatif"]

# Vectorisation BoW
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textes)

# Classification
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3)
model = MultinomialNB()
model.fit(X_train, y_train)

# Test
nouveau_texte = ["Ce film est incroyable"]
prediction = model.predict(vectorizer.transform(nouveau_texte))
print(f"Sentiment pr√©dit: {prediction[0]}")
```

---

## üìñ **Section 3 : TF-IDF - L'Intelligence du Comptage**

### üß† **Le Probl√®me avec BoW Simple**

Consid√©rez ces deux documents :
- Doc 1: "Le chat mange le poisson"
- Doc 2: "Le machine learning transforme le monde"

Avec BoW, "le" a le m√™me poids que "machine learning". Mais "le" n'apporte aucune information discriminante !

### üìä **TF-IDF : Term Frequency √ó Inverse Document Frequency**

**Philosophie :** Un mot est important s'il appara√Æt souvent dans un document (TF) mais rarement dans la collection globale (IDF).

#### **TF (Term Frequency)**
```
TF(terme, document) = Nombre d'occurrences du terme / Nombre total de mots
```

#### **IDF (Inverse Document Frequency)**
```
IDF(terme) = log(Nombre total de documents / Nombre de documents contenant le terme)
```

#### **TF-IDF Final**
```
TF-IDF(terme, document) = TF(terme, document) √ó IDF(terme)
```

### üßÆ **Calcul Manuel D√©taill√©**

```python
import math
from collections import Counter

def calculate_tf_idf_manual(documents):
    """
    Calcul TF-IDF from scratch avec explications d√©taill√©es
    """
    
    # Pr√©paration des documents
    docs_words = [doc.lower().split() for doc in documents]
    
    # Construction du vocabulaire
    all_words = set()
    for words in docs_words:
        all_words.update(words)
    vocab = sorted(list(all_words))
    
    # Calcul TF pour chaque document
    tf_matrix = []
    for words in docs_words:
        word_count = Counter(words)
        total_words = len(words)
        
        tf_vector = []
        for word in vocab:
            tf = word_count[word] / total_words
            tf_vector.append(tf)
        tf_matrix.append(tf_vector)
    
    # Calcul IDF pour chaque terme
    idf_vector = []
    total_docs = len(documents)
    
    for word in vocab:
        docs_with_word = sum(1 for words in docs_words if word in words)
        idf = math.log(total_docs / docs_with_word)
        idf_vector.append(idf)
    
    # Calcul TF-IDF final
    tfidf_matrix = []
    for tf_vector in tf_matrix:
        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]
        tfidf_matrix.append(tfidf_vector)
    
    return tfidf_matrix, vocab, tf_matrix, idf_vector

# Exemple d'utilisation
documents = [
    "Le chat mange du poisson",
    "Le chien mange des croquettes",
    "Machine learning et intelligence artificielle",
    "Python pour le machine learning"
]

tfidf_matrix, vocab, tf_matrix, idf_vector = calculate_tf_idf_manual(documents)

# Affichage des r√©sultats
print("Vocabulaire:", vocab)
print("\nScores IDF (mots rares = scores √©lev√©s):")
for word, idf in zip(vocab, idf_vector):
    print(f"{word}: {idf:.3f}")
```

### üìà **Visualisation des Scores TF-IDF**

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_tfidf(tfidf_matrix, vocab, documents):
    """
    Visualise les scores TF-IDF sous forme de heatmap
    """
    # Cr√©ation du DataFrame
    df = pd.DataFrame(tfidf_matrix, 
                     columns=vocab,
                     index=[f"Doc {i+1}" for i in range(len(documents))])
    
    # Heatmap
    plt.figure(figsize=(12, 6))
    sns.heatmap(df, annot=True, fmt='.3f', cmap='YlOrRd')
    plt.title("Scores TF-IDF par Document et Terme")
    plt.xlabel("Termes")
    plt.ylabel("Documents")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    # Top mots par document
    for i, (doc, scores) in enumerate(zip(documents, tfidf_matrix)):
        word_scores = list(zip(vocab, scores))
        top_words = sorted(word_scores, key=lambda x: x[1], reverse=True)[:3]
        print(f"\nDocument {i+1}: '{doc[:30]}...'")
        print("Mots les plus importants:")
        for word, score in top_words:
            if score > 0:
                print(f"  - {word}: {score:.3f}")
```

### üöÄ **TF-IDF avec sklearn**

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Version professionnelle
vectorizer = TfidfVectorizer(
    max_features=1000,        # Limite le vocabulaire
    stop_words='english',     # Supprime les mots vides
    ngram_range=(1, 2),       # Uni + bigrammes
    min_df=2,                 # Ignore les mots tr√®s rares
    max_df=0.95              # Ignore les mots tr√®s fr√©quents
)

# Application sur corpus
corpus = [
    "Python est parfait pour le machine learning",
    "Scikit-learn simplifie le machine learning",
    "TensorFlow pour le deep learning avanc√©",
    "Les r√©seaux de neurones transforment l'IA"
]

tfidf_matrix = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()

print("Forme de la matrice:", tfidf_matrix.shape)
print("Premiers termes:", feature_names[:10])
```

---

## üìñ **Section 4 : Word Embeddings - La R√©volution S√©mantique**

### üåü **Au-del√† du Comptage : Comprendre le Sens**

**Le probl√®me avec TF-IDF :** Il ne sait pas que "voiture" et "automobile" sont synonymes.

**La solution Word Embeddings :** Repr√©senter chaque mot par un vecteur dense qui capture son sens et ses relations avec d'autres mots.

### üß¨ **Le Miracle de Word2Vec**

Word2Vec apprend les repr√©sentations en analysant les **contextes** o√π apparaissent les mots.

**Principe :** "Les mots qui apparaissent dans des contextes similaires ont des sens similaires"

```python
# Exemples de contextes
contexts = [
    "Le [roi] r√®gne sur son royaume",
    "La [reine] gouverne avec sagesse", 
    "Le [chat] dort sur le canap√©",
    "Le [chaton] joue dans le jardin"
]

# Word2Vec va apprendre que:
# roi ‚âà reine (contexte de pouvoir)
# chat ‚âà chaton (contexte animal domestique)
```

### üéØ **Les Analogies Magiques**

```python
import gensim.downloader as api

# Chargement d'un mod√®le pr√©-entra√Æn√© fran√ßais
# Note: En pratique, vous devrez t√©l√©charger un mod√®le fran√ßais
# model = api.load('word2vec-google-news-300')

# Exemples d'analogies possibles:
# roi - homme + femme ‚âà reine
# Paris - France + Italie ‚âà Rome
# grand - plus_grand + intelligent ‚âà plus_intelligent

def test_analogies(model):
    """
    Teste des analogies avec Word2Vec
    """
    analogies = [
        ("roi", "homme", "femme"),  # ‚Üí reine
        ("Paris", "France", "Italie"),  # ‚Üí Rome
        ("grand", "petit", "haut")  # ‚Üí bas
    ]
    
    for a, b, c in analogies:
        try:
            # Calcul: a - b + c = ?
            result = model.most_similar(positive=[a, c], negative=[b], topn=1)
            print(f"{a} - {b} + {c} = {result[0][0]} (score: {result[0][1]:.3f})")
        except KeyError as e:
            print(f"Mot non trouv√© dans le vocabulaire: {e}")
```

### üõ†Ô∏è **Utilisation Pratique avec spaCy**

```python
import spacy

# Chargement du mod√®le fran√ßais avec embeddings
nlp = spacy.load("fr_core_news_md")  # Mod√®le moyen avec vecteurs

def explore_word_vectors():
    """
    Exploration des embeddings avec spaCy
    """
    
    # Mots √† analyser
    mots = ["chat", "chien", "voiture", "automobile", "heureux", "joyeux"]
    
    # Extraction des vecteurs
    vecteurs = {}
    for mot in mots:
        doc = nlp(mot)
        if doc[0].has_vector:
            vecteurs[mot] = doc[0].vector
            print(f"{mot}: vecteur de dimension {len(doc[0].vector)}")
    
    # Calcul de similarit√©s
    print("\n=== Similarit√©s ===")
    from sklearn.metrics.pairwise import cosine_similarity
    
    for i, mot1 in enumerate(mots):
        for mot2 in mots[i+1:]:
            if mot1 in vecteurs and mot2 in vecteurs:
                sim = cosine_similarity([vecteurs[mot1]], [vecteurs[mot2]])[0][0]
                print(f"{mot1} ‚Üî {mot2}: {sim:.3f}")

explore_word_vectors()
```

### üîç **Clustering S√©mantique**

```python
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def semantic_clustering(words, n_clusters=3):
    """
    Regroupe des mots par similarit√© s√©mantique
    """
    nlp = spacy.load("fr_core_news_md")
    
    # Extraction des vecteurs
    vectors = []
    valid_words = []
    
    for word in words:
        doc = nlp(word)
        if doc[0].has_vector:
            vectors.append(doc[0].vector)
            valid_words.append(word)
    
    if len(vectors) == 0:
        print("Aucun vecteur trouv√©!")
        return
    
    vectors = np.array(vectors)
    
    # Clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(vectors)
    
    # Visualisation 2D avec PCA
    pca = PCA(n_components=2)
    vectors_2d = pca.fit_transform(vectors)
    
    plt.figure(figsize=(10, 8))
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    for i in range(n_clusters):
        cluster_points = vectors_2d[clusters == i]
        cluster_words = [valid_words[j] for j in range(len(valid_words)) if clusters[j] == i]
        
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], 
                   c=colors[i], label=f'Cluster {i+1}', s=100)
        
        # Annotations
        for j, word in enumerate(cluster_words):
            plt.annotate(word, 
                        (cluster_points[j, 0], cluster_points[j, 1]),
                        xytext=(5, 5), textcoords='offset points')
    
    plt.title("Clustering S√©mantique des Mots")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # Affichage des clusters
    for i in range(n_clusters):
        cluster_words = [valid_words[j] for j in range(len(valid_words)) if clusters[j] == i]
        print(f"Cluster {i+1}: {', '.join(cluster_words)}")

# Test avec des mots vari√©s
mots_test = [
    "chat", "chien", "animal", "oiseau",
    "voiture", "automobile", "transport", "train",
    "heureux", "joyeux", "triste", "m√©lancolique",
    "ordinateur", "machine", "robot", "intelligence"
]

semantic_clustering(mots_test, n_clusters=4)
```

---

## üìñ **Section 5 : Calcul de Similarit√©**

### üìê **M√©triques de Distance et Similarit√©**

Une fois que nous avons transform√© nos textes en vecteurs, comment mesurer leur proximit√© ?

#### **1. Similarit√© Cosinus** *(La plus populaire)*

```python
import numpy as np

def cosine_similarity_manual(vec1, vec2):
    """
    Calcule la similarit√© cosinus entre deux vecteurs
    """
    # Produit scalaire
    dot_product = np.dot(vec1, vec2)
    
    # Normes des vecteurs
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    # Similarit√© cosinus
    if norm1 == 0 or norm2 == 0:
        return 0
    
    similarity = dot_product / (norm1 * norm2)
    return similarity

# Exemple pratique
texte1_vec = np.array([1, 2, 0, 1])  # Repr√©sentation TF-IDF du texte 1
texte2_vec = np.array([2, 1, 1, 0])  # Repr√©sentation TF-IDF du texte 2

similarity = cosine_similarity_manual(texte1_vec, texte2_vec)
print(f"Similarit√© cosinus: {similarity:.3f}")

# Interpr√©tation:
# 1.0 = Identiques
# 0.0 = Orthogonaux (aucune relation)
# -1.0 = Oppos√©s
```

#### **2. Distance Euclidienne**

```python
def euclidean_distance(vec1, vec2):
    """
    Calcule la distance euclidienne entre deux vecteurs
    """
    return np.sqrt(np.sum((vec1 - vec2) ** 2))

# Note: Plus la distance est petite, plus les textes sont similaires
distance = euclidean_distance(texte1_vec, texte2_vec)
print(f"Distance euclidienne: {distance:.3f}")
```

#### **3. Distance de Manhattan**

```python
def manhattan_distance(vec1, vec2):
    """
    Calcule la distance de Manhattan entre deux vecteurs
    """
    return np.sum(np.abs(vec1 - vec2))

distance_manhattan = manhattan_distance(texte1_vec, texte2_vec)
print(f"Distance de Manhattan: {distance_manhattan:.3f}")
```

### üî¨ **Comparaison Pratique des M√©triques**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances

def compare_similarity_metrics(texts):
    """
    Compare diff√©rentes m√©triques de similarit√© sur un corpus
    """
    # Vectorisation
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # Calcul des similarit√©s/distances
    cos_sim = cosine_similarity(tfidf_matrix)
    eucl_dist = euclidean_distances(tfidf_matrix)
    manh_dist = manhattan_distances(tfidf_matrix)
    
    # Affichage comparatif
    n_texts = len(texts)
    
    print("=== COMPARAISON DES M√âTRIQUES ===\n")
    
    for i in range(n_texts):
        for j in range(i+1, n_texts):
            print(f"Texte {i+1} vs Texte {j+1}:")
            print(f"  Cosinus Similarit√©: {cos_sim[i,j]:.3f}")
            print(f"  Distance Euclidienne: {eucl_dist[i,j]:.3f}")
            print(f"  Distance Manhattan: {manh_dist[i,j]:.3f}")
            print()

# Test avec exemples concrets
textes_test = [
    "Python est un excellent langage de programmation",
    "J'adore programmer en Python, c'est fantastique",
    "Java est utilis√© pour le d√©veloppement d'applications",
    "Le machine learning r√©volutionne la technologie"
]

compare_similarity_metrics(textes_test)
```

### üéØ **Applications Pratiques**

#### **Syst√®me de Recommandation Simple**

```python
def recommend_similar_articles(query, articles, top_k=3):
    """
    Recommande des articles similaires √† une requ√™te
    """
    # Pr√©paration des donn√©es
    all_texts = [query] + articles
    
    # Vectorisation
    vectorizer = TfidfVectorizer(stop_words='french')
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    
    # Calcul similarit√© avec la requ√™te (premier √©l√©ment)
    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
    
    # Tri par similarit√© d√©croissante
    similar_indices = similarities.argsort()[::-1][:top_k]
    
    # Retour des r√©sultats
    recommendations = []
    for idx in similar_indices:
        recommendations.append({
            'article': articles[idx],
            'score': similarities[idx],
            'rank': len(recommendations) + 1
        })
    
    return recommendations

# Test du syst√®me de recommandation
query = "intelligence artificielle et machine learning"

articles_db = [
    "Les r√©seaux de neurones transforment l'IA moderne",
    "Recette de cuisine: tarte aux pommes traditionnelle",
    "Deep learning pour la reconnaissance d'images",
    "Guide de voyage: visiter Paris en 3 jours",
    "Algorithmes de classification en machine learning",
    "Histoire de l'art contemporain fran√ßais"
]

recommendations = recommend_similar_articles(query, articles_db, top_k=3)

print(f"Requ√™te: '{query}'\n")
print("Recommandations:")
for rec in recommendations:
    print(f"{rec['rank']}. {rec['article']}")
    print(f"   Score: {rec['score']:.3f}\n")
```

---

## üõ†Ô∏è **Exercices Pratiques**

### üìù **Exercice 5 : Bag of Words Manuel**
**Objectif :** Impl√©menter BoW from scratch et comparer avec sklearn
**Difficult√© :** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ | **Points :** 7

#### Instructions
1. Cr√©ez une fonction `create_bow_manual()` qui transforme une liste de textes en matrice BoW
2. Testez sur 3 phrases de votre choix
3. Comparez les r√©sultats avec `CountVectorizer` de sklearn
4. Analysez les diff√©rences et expliquez pourquoi elles existent

#### Code de d√©part
```python
def create_bow_manual(documents):
    """
    Votre impl√©mentation ici
    Retourne: (matrice_bow, vocabulaire)
    """
    pass

# Tests √† effectuer
test_documents = [
    "Python est g√©nial pour programmer",
    "J'adore programmer en Python",
    "Le machine learning avec Python est fascinant"
]
```

#### Crit√®res d'√©valuation
- [ ] Fonction correctement impl√©ment√©e
- [ ] Gestion de la casse et ponctuation
- [ ] Comparaison d√©taill√©e avec sklearn
- [ ] Analyse des diff√©rences

---

### üìù **Exercice 6 : TF-IDF from Scratch**
**Objectif :** Comprendre TF-IDF en l'impl√©mentant manuellement
**Difficult√© :** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ | **Points :** 7

#### Instructions
1. Impl√©mentez les fonctions `calculate_tf()`, `calculate_idf()` et `calculate_tfidf()`
2. Testez sur un corpus de 4-5 documents
3. Cr√©ez une visualisation des scores TF-IDF
4. Identifiez les mots les plus discriminants pour chaque document

#### Code de d√©part
```python
import math
from collections import Counter

def calculate_tf(document):
    """Calcule la fr√©quence des termes"""
    pass

def calculate_idf(documents, vocabulary):
    """Calcule l'inverse document frequency"""
    pass

def calculate_tfidf(documents):
    """Fonction principale qui combine TF et IDF"""
    pass

# Corpus de test
corpus_test = [
    "Le chat mange du poisson frais",
    "Le chien mange des croquettes",
    "Machine learning et intelligence artificielle",
    "Python pour le machine learning avanc√©",
    "Les algorithmes de deep learning"
]
```

#### Crit√®res d'√©valuation
- [ ] Calculs TF et IDF corrects
- [ ] Impl√©mentation compl√®te de TF-IDF
- [ ] Visualisation claire des r√©sultats
- [ ] Analyse des mots discriminants

---

### üìù **Exercice 7 : Similarit√© Cosinus**
**Objectif :** Ma√Ætriser le calcul de similarit√© entre textes
**Difficult√© :** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ | **Points :** 7

#### Instructions
1. Cr√©ez une fonction qui compare 10 paires de phrases
2. Impl√©mentez la similarit√© cosinus from scratch
3. Cr√©ez un ranking humain vs machine des similarit√©s
4. Analysez les cas o√π l'humain et la machine divergent

#### Code de d√©part
```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

def cosine_similarity_manual(vec1, vec2):
    """Votre impl√©mentation de la similarit√© cosinus"""
    pass

def compare_similarities(phrases_pairs):
    """Compare les similarit√©s humaines vs machine"""
    pass

# Paires de phrases √† tester
test_pairs = [
    ("Python est g√©nial", "J'adore Python"),
    ("Il fait beau", "Le soleil brille"),
    ("Chat noir", "Chien blanc"),
    ("Machine learning", "Intelligence artificielle"),
    # Ajoutez 6 autres paires...
]
```

#### Crit√®res d'√©valuation
- [ ] Fonction cosinus correcte
- [ ] Comparaison humain/machine document√©e
- [ ] Analyse des divergences
- [ ] Insights sur les limites

---

### üìù **Exercice 8 : Exploration Word2Vec**
**Objectif :** D√©couvrir la puissance des word embeddings
**Difficult√© :** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ | **Points :** 7

#### Instructions
1. Chargez un mod√®le Word2Vec pr√©-entra√Æn√© fran√ßais
2. Trouvez 10 analogies qui fonctionnent bien
3. Cr√©ez un quiz "devine le mot manquant"
4. Analysez les limitations et biais du mod√®le

#### Code de d√©part
```python
import spacy

# Chargement du mod√®le
nlp = spacy.load("fr_core_news_md")

def test_analogies(word_a, word_b, word_c):
    """Teste l'analogie: a est √† b ce que c est √† ?"""
    pass

def create_word_quiz():
    """Cr√©e un quiz interactif"""
    pass

def analyze_word_clusters(words_list):
    """Analyse les clusters de mots similaires"""
    pass

# Mots √† analyser
test_words = [
    "roi", "reine", "homme", "femme",
    "Paris", "France", "Londres", "Angleterre",
    # Ajoutez d'autres mots...
]
```

#### Crit√®res d'√©valuation
- [ ] 10 analogies fonctionnelles trouv√©es
- [ ] Quiz interactif cr√©√©
- [ ] Analyse critique des biais
- [ ] Visualisation des clusters

---

## üéØ **Projet Final : D√©tecteur de Plagiat/Similarit√©**

### üèÜ **Cahier des Charges**

Cr√©ez un syst√®me complet de d√©tection de similarit√© entre textes qui :

#### **Fonctionnalit√©s Principales**
1. **Interface utilisateur** (CLI ou web avec Streamlit)
2. **Plusieurs m√©thodes** de vectorisation (BoW, TF-IDF, embeddings)
3. **Calcul de similarit√©** avec diff√©rentes m√©triques
4. **Visualisations** des r√©sultats
5. **Base de donn√©es** de documents de r√©f√©rence

#### **Sp√©cifications Techniques**

```python
class DetecteurSimilarite:
    def __init__(self, method='tfidf'):
        """
        Initialise le d√©tecteur
        method: 'bow', 'tfidf', ou 'embeddings'
        """
        pass
    
    def add_reference_document(self, text, title):
        """Ajoute un document √† la base de r√©f√©rence"""
        pass
    
    def check_similarity(self, query_text, threshold=0.7):
        """
        V√©rifie la similarit√© avec tous les documents de r√©f√©rence
        Retourne: [(doc_title, similarity_score), ...]
        """
        pass
    
    def visualize_results(self, results):
        """Cr√©e des graphiques des r√©sultats"""
        pass
    
    def generate_report(self, query_text, results):
        """G√©n√®re un rapport d√©taill√©"""
        pass
```

#### **Interface Streamlit Sugg√©r√©e**

```python
import streamlit as st

def main():
    st.title("üîç D√©tecteur de Similarit√© de Textes")
    
    # Sidebar pour configuration
    st.sidebar.header("Configuration")
    method = st.sidebar.selectbox(
        "M√©thode de vectorisation",
        ["TF-IDF", "Bag of Words", "Word Embeddings"]
    )
    
    threshold = st.sidebar.slider(
        "Seuil de similarit√©",
        0.0, 1.0, 0.7
    )
    
    # Interface principale
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("Texte √† analyser")
        query_text = st.text_area("Entrez votre texte:", height=200)
        
        if st.button("Analyser"):
            # Logique d'analyse
            pass
    
    with col2:
        st.header("R√©sultats")
        # Affichage des r√©sultats
        pass
```

#### **Dataset de Test Fourni**

- **50 articles Wikipedia** fran√ßais (sciences, histoire, litt√©rature)
- **30 essais d'√©tudiants** avec versions originales et plagiat d√©tect√©
- **100 tweets** sur des sujets vari√©s
- **20 articles de presse** sur l'actualit√© technologique

#### **Crit√®res d'√âvaluation**

| Crit√®re | Points | Description |
|---------|--------|-------------|
| **Fonctionnalit√©** | 30 | Toutes les features demand√©es |
| **Qualit√© du Code** | 20 | Code propre, comment√©, structur√© |
| **Interface** | 20 | UX intuitive et attractive |
| **Visualisations** | 15 | Graphiques informatifs |
| **Documentation** | 10 | README d√©taill√© |
| **Tests** | 5 | Cas de test vari√©s |

#### **Bonus Possibles** *(+10 points)*
- [ ] Support multilingue
- [ ] API REST
- [ ] D√©ploiement en ligne
- [ ] D√©tection de paraphrase
- [ ] Mode batch pour plusieurs fichiers

---

## üìä **Auto-√âvaluation**

### ‚úÖ **Checklist de Compr√©hension**

Avant de passer au module suivant, assurez-vous de pouvoir :

- [ ] **Expliquer** la diff√©rence entre BoW, TF-IDF et embeddings
- [ ] **Calculer** manuellement un score TF-IDF
- [ ] **Interpr√©ter** une similarit√© cosinus
- [ ] **Choisir** la bonne m√©trique selon le contexte
- [ ] **Identifier** les limitations de chaque approche
- [ ] **Impl√©menter** un syst√®me de comparaison de textes
- [ ] **Visualiser** des r√©sultats de vectorisation
- [ ] **D√©bugger** des probl√®mes de preprocessing

### üéØ **Quiz d'Auto-√âvaluation**

#### **Question 1 :** Vrai ou Faux ?
"TF-IDF donne plus d'importance aux mots rares qu'aux mots fr√©quents"

<details>
<summary>Voir la r√©ponse</summary>

**Vrai.** IDF (Inverse Document Frequency) p√©nalise les mots qui apparaissent dans beaucoup de documents, donnant plus de poids aux mots rares et discriminants.
</details>

#### **Question 2 :** Calcul pratique
Soit le vocabulaire ["chat", "mange", "poisson"] et les documents :
- Doc1: "Le chat mange"
- Doc2: "Le poisson nage"

Quelle est la repr√©sentation BoW de Doc1 ?

<details>
<summary>Voir la r√©ponse</summary>

**[1, 1, 0]** - "chat":1, "mange":1, "poisson":0
</details>

#### **Question 3 :** Analogie Word2Vec
Compl√©tez : "roi - homme + femme = ___"

<details>
<summary>Voir la r√©ponse</summary>

**reine** - C'est l'analogie classique qui d√©montre que Word2Vec capture les relations s√©mantiques.
</details>

---

## üîó **Ressources et Liens Utiles**

### üìö **Documentation Officielle**
- [Scikit-learn Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- [spaCy Word Vectors](https://spacy.io/usage/vectors-similarity)
- [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)

### üéì **Ressources P√©dagogiques**
- [Visualisation interactive TF-IDF](http://www.tfidf.com/)
- [Word2Vec expliqu√© visuellement](https://ronxin.github.io/wevi/)
- [Cours Stanford CS224N](http://web.stanford.edu/class/cs224n/)

### üîß **Outils Pratiques**
- [Datasets fran√ßais pour NLP](https://github.com/clu-ling/french-nlp-datasets)
- [Mod√®les spaCy fran√ßais](https://spacy.io/models/fr)
- [Word2Vec fran√ßais pr√©-entra√Æn√©](https://fauconnier.github.io/)

### üìñ **Lectures Approfondies**
- Paper original TF-IDF (Salton & McGill, 1983)
- Word2Vec original papers (Mikolov et al., 2013)
- "Speech and Language Processing" (Jurafsky & Martin)

---

## üöÄ **Transition vers le Module 3**

### üéØ **Ce que Vous Avez Acquis**

F√©licitations ! Vous ma√Ætrisez maintenant :
- ‚úÖ La transformation de texte en repr√©sentations num√©riques
- ‚úÖ Les techniques de vectorisation (BoW, TF-IDF, embeddings)
- ‚úÖ Le calcul de similarit√© entre documents
- ‚úÖ L'utilisation d'outils professionnels (sklearn, spaCy)

### üîÆ **Ce qui Vous Attend**

Dans le **Module 3 : Classification et Analyse de Sentiments**, vous apprendrez √† :
- üéØ Entra√Æner des mod√®les de classification sur vos vecteurs
- üòäüò° D√©tecter automatiquement les √©motions dans les textes
- üìä √âvaluer et optimiser les performances de vos mod√®les
- üè≠ Cr√©er un pipeline de production robuste

### üåâ **Pr√©paration**

Assurez-vous d'avoir :
- [ ] Valid√© tous les exercices de ce module
- [ ] Termin√© le projet d√©tecteur de similarit√©
- [ ] Compris les concepts de vectorisation
- [ ] Install√© les d√©pendances pour la classification

---

## üìû **Support et Communaut√©**

### üÜò **En Cas de Probl√®me**

1. **FAQ Module 2** : Consultez les questions fr√©quentes
2. **Debugging Guide** : Solutions aux erreurs communes
3. **Forum Communaut√©** : Posez vos questions
4. **Office Hours** : Sessions Q&R hebdomadaires

### ü§ù **Contribuer**

- Proposez des am√©liorations via GitHub Issues
- Partagez vos projets cr√©atifs
- Aidez les autres √©tudiants
- Sugg√©rez de nouveaux datasets

---

**üéâ Bravo ! Vous avez termin√© le Module 2 : Vectorisation !**

*Prochain arr√™t : Module 3 - Classification et Analyse de Sentiments* üéØ