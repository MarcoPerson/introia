# PARTIE 1 : SGD ET ADAM EN D√âTAILS

---

## üéØ 1. SGD (Stochastic Gradient Descent)

### **A. Gradient Descent Classique (Batch GD)**

**Principe :**
- Utilise **TOUS** les √©chantillons pour calculer le gradient
- Mise √† jour **une fois par √©poque**

**Algorithme :**
```python
# Pour une √©poque
for epoch in range(epochs):
    # 1. Forward sur TOUT le dataset
    y_pred = model(X_all)  # X_all : (m, n) - tous les √©chantillons
    
    # 2. Loss sur TOUT le dataset
    loss = MSE(y_pred, Y_all)
    
    # 3. Gradient calcul√© sur TOUT le dataset
    gradient = (1/m) * X_all.T @ (y_pred - Y_all)
    
    # 4. UNE SEULE mise √† jour
    theta = theta - learning_rate * gradient
```

**Dimensions :**
```
m = 10,000 √©chantillons
X_all : (10000, n)
y_pred : (10000, 1)
gradient : (n, 1)

‚Üí Calcule le gradient en utilisant les 10,000 √©chantillons √† la fois
```

**Probl√®mes :**
- ‚ùå Tr√®s lent si m est grand (millions de donn√©es)
- ‚ùå N√©cessite beaucoup de m√©moire (tout charger)
- ‚ùå Convergence lente (1 mise √† jour par √©poque)
- ‚úÖ Gradient pr√©cis et stable

---

### **B. Stochastic Gradient Descent (SGD)**

**Principe :**
- Utilise **UN SEUL** √©chantillon pour calculer le gradient
- Mise √† jour **m fois par √©poque** (une par √©chantillon)

**Algorithme :**
```python
# Pour une √©poque
for epoch in range(epochs):
    # M√©langer les donn√©es
    indices = np.random.permutation(m)
    
    # Pour CHAQUE √©chantillon
    for i in indices:
        # 1. Forward sur UN √©chantillon
        x_i = X[i]  # (n,)
        y_i = Y[i]  # (1,)
        y_pred_i = model(x_i)
        
        # 2. Loss sur UN √©chantillon
        loss_i = (y_pred_i - y_i)**2
        
        # 3. Gradient calcul√© sur UN √©chantillon
        gradient = x_i.T * (y_pred_i - y_i)
        
        # 4. Mise √† jour IMM√âDIATE
        theta = theta - learning_rate * gradient
```

**Dimensions :**
```
x_i : (n,)        - UN √©chantillon
y_i : (1,)        - UNE sortie
gradient : (n,)   - gradient pour cet √©chantillon

‚Üí m mises √† jour par √©poque
```

**Caract√©ristiques :**
- ‚úÖ Tr√®s rapide (traite 1 √©chantillon √† la fois)
- ‚úÖ Peu de m√©moire
- ‚úÖ Peut √©chapper aux minima locaux (gr√¢ce au bruit)
- ‚ùå Gradient bruit√© (forte variance)
- ‚ùå Convergence instable (oscillations)

**Visualisation de la convergence :**
```
Batch GD :     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº  (lisse, direct vers le minimum)

SGD :          ‚îÄ‚ï±‚îÄ‚ï≤‚îÄ‚ï±‚îÄ‚ñº‚ï≤‚îÄ‚ï±‚îÄ  (oscillant, bruit√©)
```

---

### **C. Mini-Batch SGD (Le plus utilis√©)**

**Principe :**
- Utilise **un petit lot** (batch) d'√©chantillons
- Compromis entre Batch GD et SGD
- **C'est ce qu'on utilise dans le TP !**

**Algorithme :**
```python
batch_size = 32  # Typiquement 16, 32, 64, 128, 256

# Pour une √©poque
for epoch in range(epochs):
    # M√©langer les donn√©es
    indices = np.random.permutation(m)
    
    # D√©couper en mini-batches
    num_batches = m // batch_size
    
    for batch_idx in range(num_batches):
        # Extraire un mini-batch
        start = batch_idx * batch_size
        end = start + batch_size
        batch_indices = indices[start:end]
        
        X_batch = X[batch_indices]  # (batch_size, n)
        Y_batch = Y[batch_indices]  # (batch_size, 1)
        
        # 1. Forward sur le batch
        y_pred_batch = model(X_batch)
        
        # 2. Loss sur le batch
        loss = MSE(y_pred_batch, Y_batch)
        
        # 3. Gradient moyenn√© sur le batch
        gradient = (1/batch_size) * X_batch.T @ (y_pred_batch - Y_batch)
        
        # 4. Mise √† jour
        theta = theta - learning_rate * gradient
```

**Dimensions :**
```
m = 10,000 √©chantillons
batch_size = 32

X_batch : (32, n)      - un mini-batch
Y_batch : (32, 1)
gradient : (n, 1)      - moyenn√© sur 32 √©chantillons

‚Üí 10000/32 ‚âà 312 mises √† jour par √©poque
```

**Avantages :**
- ‚úÖ Gradient plus stable que SGD pur (moyenn√© sur batch_size)
- ‚úÖ Plus rapide que Batch GD (plusieurs mises √† jour par √©poque)
- ‚úÖ Exploite la parall√©lisation GPU (matrices)
- ‚úÖ Bon compromis vitesse/stabilit√©

**Courbe de convergence :**
```
Loss
  |
  |  ‚ï≤
  |   ‚ï≤_
  |     ‚ï≤___
  |        ‚ï≤_____
  |             ‚ï≤_______  (oscillations mod√©r√©es)
  +‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ It√©rations
```

---

### **D. SGD avec Momentum**

**Probl√®me du SGD :**
- Oscillations dans les directions perpendiculaires au minimum
- Progression lente dans la direction du minimum

**Solution : Momentum**

**Principe :**
- Accumule un "√©lan" dans la direction des gradients pr√©c√©dents
- Comme une boule qui roule et prend de la vitesse

**Algorithme :**
```python
# Initialisation
velocity = 0
beta = 0.9  # coefficient de momentum (typiquement 0.9)

# √Ä chaque it√©ration
for epoch in range(epochs):
    for X_batch, Y_batch in dataloader:
        # Calcul du gradient
        gradient = compute_gradient(X_batch, Y_batch)
        
        # Mise √† jour de la v√©locit√© (√©lan)
        velocity = beta * velocity + (1 - beta) * gradient
        
        # Mise √† jour des param√®tres
        theta = theta - learning_rate * velocity
```

**Explication math√©matique :**
```
v_t = Œ≤¬∑v_{t-1} + (1-Œ≤)¬∑g_t

o√π :
- v_t : v√©locit√© au temps t
- g_t : gradient au temps t
- Œ≤ : facteur d'amortissement (0.9 = 90% de l'ancien √©lan conserv√©)

Œ∏_t = Œ∏_{t-1} - Œ±¬∑v_t
```

**Effet visuel :**
```
Sans momentum :  ‚îÄ‚ï±‚îÄ‚ï≤‚îÄ‚ï±‚îÄ‚ï≤‚îÄ‚ï±‚îÄ  (zigzag)

Avec momentum : ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ   (plus direct)
```

**Dimensions :**
```
gradient : (n, 1)
velocity : (n, 1)  - m√™me shape que gradient
theta : (n, 1)
```

**Avantages :**
- ‚úÖ Acc√©l√®re dans les directions consistantes
- ‚úÖ Amortit les oscillations
- ‚úÖ Converge plus vite
- ‚úÖ Peut franchir de petits plateaux

---

## üöÄ 2. ADAM (Adaptive Moment Estimation)

**Adam = SGD + Momentum + Adaptation du learning rate**

C'est l'**optimiseur le plus utilis√©** en deep learning !

### **A. Principe**

Adam combine **deux id√©es** :
1. **Momentum** : accumule l'√©lan des gradients
2. **RMSprop** : adapte le learning rate pour chaque param√®tre

**Pourquoi c'est puissant ?**
- Certains param√®tres ont besoin d'un grand learning rate
- D'autres ont besoin d'un petit learning rate
- Adam s'adapte automatiquement !

---

### **B. Algorithme Complet**

```python
# Hyperparam√®tres
learning_rate = 0.001  # Œ±
beta1 = 0.9            # pour le momentum (first moment)
beta2 = 0.999          # pour RMSprop (second moment)
epsilon = 1e-8         # pour √©viter division par z√©ro

# Initialisation
m = 0  # first moment (momentum)
v = 0  # second moment (variance)
t = 0  # compteur d'it√©rations

# √Ä chaque it√©ration
for epoch in range(epochs):
    for X_batch, Y_batch in dataloader:
        t += 1
        
        # 1. Calcul du gradient
        gradient = compute_gradient(X_batch, Y_batch)  # g_t
        
        # 2. Mise √† jour du first moment (moyenne mobile du gradient)
        m = beta1 * m + (1 - beta1) * gradient
        
        # 3. Mise √† jour du second moment (moyenne mobile du carr√© du gradient)
        v = beta2 * v + (1 - beta2) * gradient**2
        
        # 4. Correction du biais (bias correction)
        m_hat = m / (1 - beta1**t)
        v_hat = v / (1 - beta2**t)
        
        # 5. Mise √† jour des param√®tres
        theta = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
```

---

### **C. D√©cortiquons Chaque √âtape**

#### **√âtape 2 : First Moment (m)**

```
m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑g_t

C'est une moyenne mobile exponentielle des gradients
‚Üí √âquivalent au momentum
‚Üí "Dans quelle direction dois-je aller ?"
```

**Dimensions :**
```
gradient : (n, 1)
m : (n, 1)  - m√™me shape que les param√®tres
```

**Exemple num√©rique :**
```python
beta1 = 0.9
m = 0
gradients = [1.0, 1.2, 0.8, 1.1]

It√©ration 1: m = 0.9*0 + 0.1*1.0 = 0.1
It√©ration 2: m = 0.9*0.1 + 0.1*1.2 = 0.21
It√©ration 3: m = 0.9*0.21 + 0.1*0.8 = 0.269
It√©ration 4: m = 0.9*0.269 + 0.1*1.1 = 0.352

‚Üí m accumule une "m√©moire" des gradients pass√©s
```

---

#### **√âtape 3 : Second Moment (v)**

```
v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑g_t¬≤

C'est une moyenne mobile des carr√©s des gradients
‚Üí Mesure la "variance" des gradients
‚Üí "√Ä quelle vitesse dois-je aller ?"
```

**Dimensions :**
```
gradient**2 : (n, 1)  - carr√© √©l√©ment par √©l√©ment
v : (n, 1)
```

**Exemple num√©rique :**
```python
beta2 = 0.999
v = 0
gradients = [1.0, 1.2, 0.8, 1.1]

It√©ration 1: v = 0.999*0 + 0.001*1.0¬≤ = 0.001
It√©ration 2: v = 0.999*0.001 + 0.001*1.2¬≤ = 0.002439
It√©ration 3: v = 0.999*0.002439 + 0.001*0.8¬≤ = 0.003077
It√©ration 4: v = 0.999*0.003077 + 0.001*1.1¬≤ = 0.004285

‚Üí v suit la variance des gradients
```

---

#### **√âtape 4 : Bias Correction**

**Pourquoi ?**

Au d√©but, m et v sont initialis√©s √† 0, donc ils sont **biais√©s** vers 0.

**Exemple :**
```python
# Premier gradient
g1 = 1.0
m1 = 0.9*0 + 0.1*1.0 = 0.1  # Devrait √™tre proche de 1.0, pas 0.1 !
```

**Correction :**
```
m_hat = m / (1 - Œ≤‚ÇÅ^t)
v_hat = v / (1 - Œ≤‚ÇÇ^t)

Au d√©but (t petit) : correction forte
Plus tard (t grand) : correction n√©gligeable
```

**Exemple num√©rique :**
```python
beta1 = 0.9
t = 1: m_hat = m / (1 - 0.9^1) = m / 0.1 = 10¬∑m  ‚Üê grosse correction
t = 2: m_hat = m / (1 - 0.9^2) = m / 0.19 = 5.26¬∑m
t = 10: m_hat = m / (1 - 0.9^10) = m / 0.651 = 1.54¬∑m
t = 100: m_hat = m / (1 - 0.9^100) ‚âà m  ‚Üê correction n√©gligeable
```

---

#### **√âtape 5 : Mise √† Jour Adaptive**

```
Œ∏_t = Œ∏_{t-1} - Œ± ¬∑ m_hat / (‚àöv_hat + Œµ)
```

**D√©composons :**

```
Learning rate adaptatif = Œ± / (‚àöv_hat + Œµ)

- Si v_hat grand (gradient varie beaucoup) ‚Üí petit pas
- Si v_hat petit (gradient stable) ‚Üí grand pas
```

**Exemple num√©rique :**
```python
alpha = 0.001
epsilon = 1e-8

# Param√®tre 1 : gradient stable
m_hat_1 = 0.5
v_hat_1 = 0.01
update_1 = 0.001 * 0.5 / (sqrt(0.01) + 1e-8)
         = 0.001 * 0.5 / 0.1
         = 0.005  ‚Üê grand pas

# Param√®tre 2 : gradient tr√®s variable
m_hat_2 = 0.5
v_hat_2 = 4.0
update_2 = 0.001 * 0.5 / (sqrt(4.0) + 1e-8)
         = 0.001 * 0.5 / 2.0
         = 0.00025  ‚Üê petit pas

‚Üí Adam adapte automatiquement la taille du pas !
```

---

### **D. Code Complet d'Adam**

```python
class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.t = 0
        self.m = {}  # first moments
        self.v = {}  # second moments
    
    def update(self, params, grads):
        """
        params : dict {nom: theta}
        grads : dict {nom: gradient}
        """
        self.t += 1
        
        for name in params:
            # Initialisation si premi√®re fois
            if name not in self.m:
                self.m[name] = np.zeros_like(params[name])
                self.v[name] = np.zeros_like(params[name])
            
            # R√©cup√©rer gradient
            g = grads[name]
            
            # Mise √† jour first moment
            self.m[name] = self.beta1 * self.m[name] + (1 - self.beta1) * g
            
            # Mise √† jour second moment
            self.v[name] = self.beta2 * self.v[name] + (1 - self.beta2) * g**2
            
            # Bias correction
            m_hat = self.m[name] / (1 - self.beta1**self.t)
            v_hat = self.v[name] / (1 - self.beta2**self.t)
            
            # Mise √† jour des param√®tres
            params[name] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

# Utilisation
optimizer = AdamOptimizer(learning_rate=0.001)

for epoch in range(epochs):
    for X_batch, Y_batch in dataloader:
        # Forward
        y_pred = model.forward(X_batch)
        
        # Backward
        grads = model.backward(X_batch, Y_batch)
        
        # Update avec Adam
        optimizer.update(model.params, grads)
```

---

### **E. Dimensions dans Adam**

```python
# Pour un r√©seau : input(10) ‚Üí hidden(50) ‚Üí output(1)

# Param√®tres
W1 : (50, 10)
b1 : (50, 1)
W2 : (1, 50)
b2 : (1, 1)

# Gradients (m√™me shape que param√®tres)
dW1 : (50, 10)
db1 : (50, 1)
dW2 : (1, 50)
db2 : (1, 1)

# Adam stocke pour CHAQUE param√®tre :
m['W1'] : (50, 10)  - first moment de W1
v['W1'] : (50, 10)  - second moment de W1
m['b1'] : (50, 1)   - first moment de b1
v['b1'] : (50, 1)   - second moment de b1
...

‚Üí Adam double l'utilisation m√©moire !
```

---

## üìä 3. COMPARAISON SGD vs ADAM

### **A. Tableau Comparatif**

| Aspect | SGD (+ Momentum) | Adam |
|--------|------------------|------|
| **Learning rate** | Fixe (ou scheduler manuel) | Adaptatif par param√®tre |
| **Convergence** | Plus lente au d√©but | Tr√®s rapide au d√©but |
| **M√©moire** | m stocke seulement velocity | m + v (√ó2 m√©moire) |
| **Hyperparam√®tres** | Œ±, Œ≤ (2) | Œ±, Œ≤‚ÇÅ, Œ≤‚ÇÇ, Œµ (4) |
| **Stabilit√©** | Sensible au LR | Plus robuste |
| **G√©n√©ralisation** | Souvent meilleure | Peut sur-ajuster |
| **Popularit√©** | Recherche | Production |

---

### **B. Visualisation de la Convergence**

```
Loss
  |
  | SGD sans momentum
  | ‚ï≤  ‚ï±‚ï≤  ‚ï±‚ï≤
  |  ‚ï≤‚ï±  ‚ï≤‚ï±  ‚ï≤___
  |            ‚ï≤___
  |
  | SGD avec momentum
  |  ‚ï≤
  |   ‚ï≤____
  |       ‚ï≤____
  |
  | Adam
  |  ‚ï≤____
  |      ‚ï≤_______
  +‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ It√©rations
```

**Adam converge plus vite mais peut osciller pr√®s du minimum**

---

### **C. Quand Utiliser Quoi ?**

**Utilisez SGD (+Momentum) si :**
- ‚úÖ Vous voulez la meilleure g√©n√©ralisation possible
- ‚úÖ Vous pouvez tuner finement le learning rate
- ‚úÖ Vous avez du temps pour l'entra√Ænement
- ‚úÖ Dataset petit/moyen
- **Exemple :** Recherche, comp√©titions Kaggle

**Utilisez Adam si :**
- ‚úÖ Vous voulez converger rapidement
- ‚úÖ Vous n'avez pas le temps de tuner
- ‚úÖ Dataset tr√®s grand
- ‚úÖ R√©seau tr√®s profond
- ‚úÖ Prototypage rapide
- **Exemple :** Production, deadline serr√©e

---

### **D. Exemple Concret sur le TP**

```python
# ===== AVEC SGD =====
optimizer_sgd = torch.optim.SGD(
    model.parameters(), 
    lr=0.01,           # Doit √™tre choisi avec soin !
    momentum=0.9,
    weight_decay=0.01
)

# Entra√Ænement
for epoch in range(5000):  # Peut n√©cessiter beaucoup d'√©poques
    ...
    optimizer_sgd.step()

# R√©sultat : convergence lente mais stable
# N√©cessite tuning de lr (0.1 trop grand, 0.001 trop petit, 0.01 OK)

# ===== AVEC ADAM =====
optimizer_adam = torch.optim.Adam(
    model.parameters(),
    lr=0.001,          # Valeur par d√©faut marche souvent
    betas=(0.9, 0.999),  # Valeurs par d√©faut
    weight_decay=0.01
)

# Entra√Ænement
for epoch in range(2000):  # Converge plus vite
    ...
    optimizer_adam.step()

# R√©sultat : convergence rapide, moins de tuning n√©cessaire
```

**Comparaison des courbes :**
```python
import matplotlib.pyplot as plt

epochs_sgd = range(5000)
losses_sgd = [...]  # D√©croissance progressive

epochs_adam = range(2000)
losses_adam = [...]  # D√©croissance rapide puis plateau

plt.plot(epochs_sgd, losses_sgd, label='SGD')
plt.plot(epochs_adam, losses_adam, label='Adam')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.yscale('log')
plt.show()
```

---

# PARTIE 2 : R√âSEAUX PROFONDS ET FONCTIONS D'ACTIVATION

---

## üèóÔ∏è 1. POURQUOI DES R√âSEAUX PROFONDS ?

### **A. R√©seau Peu Profond vs Profond**

**R√©seau peu profond (shallow) :**
```
Input ‚Üí Hidden (large) ‚Üí Output

Exemple : 100 ‚Üí 1000 neurones ‚Üí 10
```

**R√©seau profond (deep) :**
```
Input ‚Üí Hidden1 ‚Üí Hidden2 ‚Üí Hidden3 ‚Üí Output

Exemple : 100 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 10
```

---

### **B. Th√©or√®me d'Approximation Universelle**

**√ânonc√© :**
> Un r√©seau avec UNE SEULE couche cach√©e (suffisamment grande) peut approximer n'importe quelle fonction continue.

**Alors pourquoi aller plus profond ?**

**R√©ponse : Efficacit√© !**

---

### **C. Exemple Concret : Reconnaissance d'Images**

**T√¢che :** Classifier une image de chat vs chien

**R√©seau peu profond :**
```
Image (28√ó28 pixels) ‚Üí 10,000 neurones ‚Üí Chat/Chien

Probl√®mes :
- Doit apprendre TOUS les patterns en une √©tape
- Chaque neurone voit l'image ENTI√àRE
- Pas de r√©utilisation des motifs
- N√©cessite √©norm√©ment de param√®tres
```

**R√©seau profond :**
```
Image ‚Üí Couche 1 : d√©tecte les bords
      ‚Üí Couche 2 : d√©tecte les formes (oreilles, yeux)
      ‚Üí Couche 3 : d√©tecte les parties (t√™te, pattes)
      ‚Üí Couche 4 : d√©tecte l'animal complet
      ‚Üí Output : Chat/Chien

Avantages :
- Hi√©rarchie de repr√©sentations
- R√©utilisation des features
- Moins de param√®tres
- Meilleure g√©n√©ralisation
```

---

### **D. Analogie : Construire un Ch√¢teau**

**Approche peu profonde :**
```
Un seul ouvrier g√©ant qui doit tout faire en une fois
‚Üí Tr√®s difficile, inefficace
```

**Approche profonde :**
```
Couche 1 : Ouvriers qui posent les fondations
Couche 2 : Ouvriers qui montent les murs
Couche 3 : Ouvriers qui posent le toit
Couche 4 : Ouvriers qui d√©corent

‚Üí Chaque √©tape sp√©cialis√©e, efficace, r√©utilisable
```

---

### **E. Comparaison Nombre de Param√®tres**

**Pour la m√™me capacit√© d'approximation :**

```python
# R√©seau peu profond
shallow = [100, 10000, 10]
params_shallow = 100*10000 + 10000*10 = 1,100,000 param√®tres

# R√©seau profond
deep = [100, 256, 128, 64, 10]
params_deep = 100*256 + 256*128 + 128*64 + 64*10
            = 25,600 + 32,768 + 8,192 + 640
            = 67,200 param√®tres

R√©duction : √ó16 moins de param√®tres !
```

**Le r√©seau profond est BEAUCOUP plus efficace**

---

### **F. Repr√©sentations Hi√©rarchiques**

**Dans un r√©seau profond, chaque couche apprend des concepts de plus en plus abstraits :**

```
R√âSEAU DE RECONNAISSANCE FACIALE

Input : Image 256√ó256

Couche 1 (basse) : D√©tecte les bords
‚îÇ ‚ï±  ‚îÇ  ‚ï≤  ‚îÄ  |  /

Couche 2 : Combine les bords en formes
‚óã  ‚ñ°  ‚ñ≥  ‚óá

Couche 3 : D√©tecte les parties du visage
üëÅ  üëÉ  üëÑ  üëÇ

Couche 4 : Assemble en visages
üë§  üë§  üë§

Output : Identit√© de la personne
```

**Preuve exp√©rimentale :**
On peut visualiser ce que chaque couche "voit" en utilisant des techniques comme :
- Activation Maximization
- Grad-CAM
- Feature Visualization

---

## üé® 2. FONCTIONS D'ACTIVATION

### **A. Pourquoi les Activations sont Essentielles**

**Sans activation (tout lin√©aire) :**

```python
# R√©seau √† 3 couches
z1 = W1 @ x + b1
z2 = W2 @ z1 + b2
z3 = W3 @ z2 + b3

# D√©veloppons
z3 = W3 @ (W2 @ (W1 @ x + b1) + b2) + b3
   = W3 @ W2 @ W1 @ x + W3 @ W2 @ b1 + W3 @ b2 + b3
   = W_combined @ x + b_combined

‚Üí C'est √©quivalent √† UNE SEULE couche lin√©aire !
```

**Avec activation (non-lin√©aire) :**

```python
z1 = W1 @ x + b1
a1 = ReLU(z1)        # ‚Üê Non-lin√©arit√© !
z2 = W2 @ a1 + b2
a2 = ReLU(z2)        # ‚Üê Non-lin√©arit√© !
z3 = W3 @ a2 + b3

‚Üí Impossible de simplifier en une couche
‚Üí Peut apprendre des fonctions complexes
```

**R√àGLE D'OR : Sans activation, un r√©seau profond = 1 couche lin√©aire**

---

### **B. Catalogue des Fonctions d'Activation**

#### **1. ReLU (Rectified Linear Unit)** ‚≠ê **LA PLUS UTILIS√âE**

```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
```

**Graphique :**
```
    |     /
  2 |    /
  1 |   /
  0 |  /
-1  | /___________
    -2  -1  0  1  2
```

**Propri√©t√©s :**
- ‚úÖ Tr√®s simple : `max(0, x)`
- ‚úÖ Calcul ultra-rapide
- ‚úÖ Pas de saturation pour x > 0
- ‚úÖ Sparsit√© : environ 50% des neurones √† 0
- ‚ùå "Dying ReLU" : neurones qui ne s'activent jamais

**Quand l'utiliser :**
- Couches cach√©es de TOUS les r√©seaux (par d√©faut)
- CNN, ResNet, Transformers

**Dimensions :**
```python
z : (n, m)  # n neurones, m √©chantillons
a = relu(z) : (n, m)  # √©l√©ment par √©l√©ment
```

**Exemple num√©rique :**
```python
z = np.array([[-2, -1, 0, 1, 2]])
a = relu(z)
# a = [[0, 0, 0, 1, 2]]

# 60% des valeurs sont devenues 0 ‚Üí sparsit√©
```

---

#### **2. Leaky ReLU**

```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1.0, alpha)
```

**Graphique :**
```
    |     /
  2 |    /
  1 |   /
  0 |  /
-1  | ‚ï±___________
    -2  -1  0  1  2
```

**Propri√©t√©s :**
- ‚úÖ R√©sout le probl√®me de dying ReLU
- ‚úÖ Garde un petit gradient pour x < 0
- Œ± typique = 0.01

**Quand l'utiliser :**
- Quand ReLU cause des dying neurons
- GAN (Generative Adversarial Networks)

**Exemple num√©rique :**
```python
alpha = 0.01
z = np.array([[-2, -1, 0, 1, 2]])
a = leaky_relu(z, alpha)
# a = [[-0.02, -0.01, 0, 1, 2]]
```

---

#### **3. Sigmoid**

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)
```

**Graphique :**
```
  1 |        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |      /
0.5 |    /
    |  /
  0 |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   -5  -2  0  2  5
```

**Propri√©t√©s :**
- ‚úÖ Sortie entre 0 et 1
- ‚úÖ Interpr√©table comme probabilit√©
- ‚ùå Vanishing gradient pour |x| > 3
- ‚ùå Sortie non centr√©e en 0
- ‚ùå Calcul exponentiel co√ªteux

**Quand l'utiliser :**
- **Couche de sortie** pour classification binaire
- LSTM/GRU (gates)
- **JAMAIS dans les couches cach√©es** (sauf cas tr√®s sp√©cifiques)

**Exemple num√©rique :**
```python
z = np.array([[-5, -2, 0, 2, 5]])
a = sigmoid(z)
# a = [[0.007, 0.119, 0.5, 0.881, 0.993]]

# Gradient pour x=-5
grad = sigmoid_derivative(-5)  
# grad ‚âà 0.0066 ‚Üí quasi 0 ! (vanishing gradient)
```

---

#### **4. Tanh (Tangente Hyperbolique)**

```python
def tanh(x):
    return np.tanh(x)
    # ou : (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

def tanh_derivative(x):
    return 1 - np.tanh(x)**2
```

**Graphique :**
```
  1 |        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |      /
  0 |    /
    |  /
 -1 |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   -5  -2  0  2  5
```

**Propri√©t√©s :**
- ‚úÖ Sortie entre -1 et 1
- ‚úÖ Centr√©e en 0 (meilleure que sigmoid)
- ‚ùå Vanishing gradient pour |x| > 2
- ‚ùå Calcul exponentiel co√ªteux

**Quand l'utiliser :**
- LSTM/RNN (cellules)
- Quand on veut des sorties centr√©es

**Exemple num√©rique :**
```python
z = np.array([[-2, -1, 0, 1, 2]])
a = tanh(z)
# a = [[-0.964, -0.762, 0, 0.762, 0.964]]
```

---

#### **5. Softmax** (Couche de sortie seulement)

```python
def softmax(z):
    """
    z : (n_classes, m_samples)
    """
    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # stabilit√© num√©rique
    return exp_z / np.sum(exp_z, axis=0, keepdims=True)
```

**Propri√©t√©s :**
- Transforme des scores en probabilit√©s
- Œ£·µ¢ softmax(z)·µ¢ = 1
- Chaque sortie ‚àà [0, 1]

**Quand l'utiliser :**
- **Couche de sortie** pour classification multi-classes

**Exemple num√©rique :**
```python
# 3 classes, 2 √©chantillons
z = np.array([[2.0, 1.0],
              [1.0, 0.0],
              [0.1, 2.0]])  # (3, 2)

probs = softmax(z)
# [[0.659, 0.259],   # Probabilit√© classe 1
#  [0.242, 0.095],   # Probabilit√© classe 2
#  [0.099, 0.646]]   # Probabilit√© classe 3

# V√©rification
print(np.sum(probs, axis=0))
# [1.0, 1.0] ‚úì

# √âchantillon 1 : 65.9% classe 1, 24.2% classe 2, 9.9% classe 3
# √âchantillon 2 : 25.9% classe 1, 9.5% classe 2, 64.6% classe 3
```

---

### **C. Probl√®me du Vanishing Gradient**

**Qu'est-ce que c'est ?**

Dans un r√©seau profond, les gradients deviennent de plus en plus petits en remontant vers les premi√®res couches.

**Pourquoi ?**

```
Gradient = ‚àÇL/‚àÇW1 = ‚àÇL/‚àÇa3 √ó ‚àÇa3/‚àÇz3 √ó ‚àÇz3/‚àÇa2 √ó ‚àÇa2/‚àÇz2 √ó ‚àÇz2/‚àÇa1 √ó ‚àÇa1/‚àÇz1 √ó ‚àÇz1/‚àÇW1
                                ‚Üë          ‚Üë          ‚Üë
                          œÉ'(z3)     œÉ'(z2)     œÉ'(z1)

Si œÉ = sigmoid : œÉ'(z) ‚â§ 0.25 pour tout z

Donc : gradient ‚àù 0.25 √ó 0.25 √ó 0.25 = 0.0156
```

**Avec un r√©seau de 10 couches :**
```
gradient ‚àù 0.25^10 = 0.0000001 ‚Üí quasi 0 !
```

**Cons√©quence :** Les premi√®res couches n'apprennent presque pas.

**Solution : ReLU !**
```
ReLU'(z) = 1 si z > 0
         = 0 si z < 0

Pas de saturation pour z > 0 !
Gradients passent sans diminution.
```

---

### **D. Influence des Activations sur l'Apprentissage**

**Exp√©rience : R√©seau profond (10 couches) sur MNIST**

```python
# Avec Sigmoid
model_sigmoid = [784, 128, 128, 128, ..., 10]  # 10 couches
activations = sigmoid

# Entra√Ænement
Epoch 10: Loss = 2.3, Accuracy = 10% (pas mieux que hasard)
Epoch 100: Loss = 2.3, Accuracy = 11%
‚Üí N'apprend PAS (vanishing gradient)

# Avec ReLU
model_relu = [784, 128, 128, 128, ..., 10]  # 10 couches
activations = ReLU

# Entra√Ænement
Epoch 10: Loss = 0.5, Accuracy = 85%
Epoch 100: Loss = 0.05, Accuracy = 98%
‚Üí Apprend TR√àS BIEN
```

---

### **E. Tableau R√©capitulatif**

| Activation | O√π l'utiliser | Avantages | Inconv√©nients |
|------------|---------------|-----------|---------------|
| **ReLU** | Couches cach√©es (d√©faut) | Rapide, pas de vanishing | Dying neurons |
| **Leaky ReLU** | Couches cach√©es (si dying ReLU) | R√©sout dying ReLU | Un peu plus lent |
| **Sigmoid** | Output (classification binaire) | Probabilit√© | Vanishing gradient |
| **Tanh** | RNN/LSTM | Centr√© en 0 | Vanishing gradient |
| **Softmax** | Output (multi-classes) | Distribution de prob | - |
| **Linear** | Output (r√©gression) | Pas de limite | Pas de non-lin√©arit√© |

---

## üéØ 3. R√âSUM√â FINAL

### **SGD vs Adam**

```python
# Pour le TP (r√©gression simple)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# Fonctionne bien, n√©cessite tuning de lr

# Pour un gros projet (CNN, Transformer)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
# Converge plus vite, moins de tuning
```

### **Profondeur**

- R√©seau peu profond : moins de param√®tres mais moins efficace
- R√©seau profond : repr√©sentations hi√©rarchiques, plus efficace
- **R√®gle** : Commencer avec 2-3 couches, augmenter si n√©cessaire

### **Activations**

- **Couches cach√©es** : ReLU (d√©faut) ou Leaky ReLU
- **Output r√©gression** : Linear (pas d'activation)
- **Output classification binaire** : Sigmoid
- **Output classification multi-classes** : Softmax

**Voil√† ! Tout est clair maintenant ?** üöÄ
