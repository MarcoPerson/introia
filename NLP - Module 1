ğŸ“ Module 1 : Anatomie du Texte Apprendre Ã  dÃ©couper le langage humain pour les machines ğŸ¬ VidÃ©o d'Introduction (5 minutes) ğŸ“º Regarder la VidÃ©o Intro 

ğŸ’¡ RÃ©sumÃ© de la vidÃ©o :

"DÃ©couvrez pourquoi Siri vous comprend parfois si mal ! Dans ce module, nous allons dÃ©cortiquer le texte comme un chirurgien pour comprendre comment transformer des mots en quelque chose que les machines peuvent digÃ©rer. Vous allez apprendre Ã  'dÃ©couper' intelligemment le langage humain et nettoyer vos donnÃ©es textuelles comme un pro. Ã€ la fin, vous aurez crÃ©Ã© votre propre explorateur de texte qui rÃ©vÃ¨le tous les secrets cachÃ©s dans n'importe quel document !"

ğŸ¯ Objectifs d'Apprentissage 

Ã€ la fin de ce module, vous serez capable de :

âœ… Comprendre pourquoi les machines ont du mal avec le langage humain âœ… MaÃ®triser la tokenisation et ses dÃ©fis âœ… ImplÃ©menter des pipelines de preprocessing robustes âœ… Utiliser spaCy et NLTK efficacement âœ… DÃ©boguer les problÃ¨mes courants de traitement de texte âœ… CrÃ©er un explorateur de texte interactif ğŸ“š Plan du Module Section Contenu DurÃ©e DifficultÃ© 01 Introduction au ProblÃ¨me 45 min â­â˜†â˜† 02 Tokenisation Intelligente 60 min â­â­â˜† 03 Preprocessing et Nettoyage 60 min â­â­â˜† 04 Outils spaCy et NLTK 45 min â­â­â˜† Exercices 4 Exercices Pratiques 120 min â­â­â­ Projet Explorateur de Texte 90 min â­â­â­ 

â±ï¸ DurÃ©e totale estimÃ©e : 6-7 heures

ğŸ“– 01. Introduction au ProblÃ¨me ğŸ¤” Pourquoi les Machines DÃ©testent le Langage Humain ? 

Imaginez que vous devez expliquer Ã  un extraterrestre ce que signifie "Il fait un temps de chien" quand il pleut. Pas Ã©vident, non ? C'est exactement le problÃ¨me des machines avec notre langage !

Le FossÃ© Texte â†” Nombres # Ce que nous voyons texte_humain = "Salut ! Comment Ã§a va ? ğŸ˜Š" # Ce que voit l'ordinateur (reprÃ©sentation ASCII) [83, 97, 108, 117, 116, 32, 33, 32, 67, 111, 109, 109, 101, 110, 116, ...] 

ğŸ¯ ProblÃ¨mes majeurs :

AmbiguÃ¯tÃ© : "La poule du pot" (repas vs rÃ©cipient ?) Contexte : "Il est cool" vs "Il fait cool" Variations : "super", "gÃ©nial", "fantastique" = mÃªme sens Erreurs : "bjr", "slt", "cc" = variations informelles Multilingue : mÃ©langes franÃ§ais/anglais/argot ğŸ§ª ExpÃ©rience : Tester Google Translate 

Essayez de traduire ces phrases et observez les erreurs :

1. "Je suis dans le rouge ce mois-ci" 2. "Il a pris la mouche" 3. "C'est du chinois pour moi" 4. "Elle a un poil dans la main" 

ğŸ’¡ Analyse : Google Translate traduit littÃ©ralement car il ne comprend pas les expressions idiomatiques !

ğŸ¯ Notre Mission 

Transformer ce chaos linguistique en donnÃ©es exploitables par les machines. C'est le prÃ©alable OBLIGATOIRE Ã  toute application NLP !

ğŸ”ª 02. Tokenisation Intelligente ğŸ“š Qu'est-ce que la Tokenisation ? 

DÃ©finition : DÃ©couper un texte en unitÃ©s plus petites (tokens) : mots, phrases, caractÃ¨res, etc.

Analogie : Comme dÃ©couper les ingrÃ©dients avant de cuisiner ! ğŸ‘¨â€ğŸ³

ğŸš« L'Approche NaÃ¯ve (qui ne marche pas) # MÃ©thode dÃ©butant (MAUVAISE) texte = "Bonjour, comment allez-vous ? J'espÃ¨re que Ã§a va !" tokens_naifs = texte.split(" ") print(tokens_naifs) # RÃ©sultat : ['Bonjour,', 'comment', 'allez-vous', '?', "J'espÃ¨re", ...] 

ğŸ”¥ ProblÃ¨mes identifiÃ©s :

Ponctuation collÃ©e aux mots Contractions mal gÃ©rÃ©es Majuscules conservÃ©es Espaces multiples ignorÃ©s âœ… L'Approche Intelligente import spacy # Chargement du modÃ¨le franÃ§ais nlp = spacy.load("fr_core_news_sm") def tokeniser_intelligemment(texte): """Tokenise un texte avec spaCy""" doc = nlp(texte) tokens = [] for token in doc: if not token.is_space: # Ignorer les espaces tokens.append({ 'texte': token.text, 'lemme': token.lemma_, 'pos': token.pos_, 'est_ponctuation': token.is_punct, 'est_stop_word': token.is_stop }) return tokens # Test texte = "Bonjour, comment allez-vous ? J'espÃ¨re que Ã§a va !" tokens = tokeniser_intelligemment(texte) for token in tokens: print(f"{token['texte']:15} | {token['lemme']:15} | {token['pos']}") 

ğŸ“Š RÃ©sultat attendu :

Bonjour | bonjour | INTJ , | , | PUNCT comment | comment | ADV allez | aller | VERB - | - | PUNCT vous | vous | PRON ? | ? | PUNCT J' | je | PRON espÃ¨re | espÃ©rer | VERB que | que | SCONJ Ã§a | Ã§a | PRON va | aller | VERB ! | ! | PUNCT ğŸ¯ Types de Tokenisation 1. Tokenisation par Mots # Standard pour la plupart des applications doc = nlp("Python est gÃ©nial pour le NLP !") mots = [token.text for token in doc if not token.is_punct] # RÃ©sultat : ['Python', 'est', 'gÃ©nial', 'pour', 'le', 'NLP'] 2. Tokenisation par Phrases # Utile pour l'analyse de documents longs texte = "Python est super. J'adore ce langage ! Et vous ?" doc = nlp(texte) phrases = [sent.text for sent in doc.sents] # RÃ©sultat : ['Python est super.', "J'adore ce langage !", 'Et vous ?'] 3. Tokenisation par CaractÃ¨res # Pour les langues sans espaces (chinois) ou l'analyse fine mot = "gÃ©nial" caracteres = list(mot) # RÃ©sultat : ['g', 'Ã©', 'n', 'i', 'a', 'l'] ğŸ”§ Gestion des Cas SpÃ©ciaux URLs et Mentions import re def nettoyer_urls_mentions(texte): """Remplace URLs et mentions par des tokens spÃ©ciaux""" # URLs texte = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', texte) # Mentions Twitter texte = re.sub(r'@\w+', '<MENTION>', texte) # Hashtags texte = re.sub(r'#\w+', '<HASHTAG>', texte) return texte # Test tweet = "Regardez cette vidÃ©o https://youtube.com/watch?v=123 @elonmusk #IA #cool" tweet_propre = nettoyer_urls_mentions(tweet) print(tweet_propre) # RÃ©sultat : "Regardez cette vidÃ©o <URL> <MENTION> <HASHTAG> <HASHTAG>" Ã‰mojis et CaractÃ¨res SpÃ©ciaux import emoji def gerer_emojis(texte): """Convertit les Ã©mojis en texte descriptif""" return emoji.demojize(texte, language='fr') # Test texte_emoji = "J'adore Python ! ğŸ˜ğŸ" texte_sans_emoji = gerer_emojis(texte_emoji) print(texte_sans_emoji) # RÃ©sultat : "J'adore Python ! :visage_souriant_avec_des_yeux_en_forme_de_cÅ“ur::serpent:" ğŸ§¹ 03. Preprocessing et Nettoyage ğŸ¯ Objectif du Preprocessing 

Transformer un texte "sale" en texte "propre" et standardisÃ© pour l'analyse.

Principe : Plus vos donnÃ©es sont propres, meilleurs seront vos rÃ©sultats !

ğŸ”§ Pipeline de Nettoyage Standard import re import string from unidecode import unidecode class NettoyeurTexte: def __init__(self): self.nlp = spacy.load("fr_core_news_sm") def nettoyer_complet(self, texte): """Pipeline complet de nettoyage""" # Ã‰tape 1 : Normalisation de base texte = self.normaliser_base(texte) # Ã‰tape 2 : Gestion des entitÃ©s spÃ©ciales texte = self.gerer_entites_speciales(texte) # Ã‰tape 3 : Tokenisation et lemmatisation tokens = self.tokeniser_et_lemmatiser(texte) # Ã‰tape 4 : Filtrage tokens = self.filtrer_tokens(tokens) return tokens def normaliser_base(self, texte): """Normalisation basique du texte""" # Conversion en minuscules texte = texte.lower() # Suppression des accents (optionnel) # texte = unidecode(texte) # Suppression des caractÃ¨res de contrÃ´le texte = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', texte) # Normalisation des espaces texte = re.sub(r'\s+', ' ', texte) return texte.strip() def gerer_entites_speciales(self, texte): """Gestion des URLs, emails, etc.""" # URLs texte = re.sub(r'http[s]?://\S+', '<URL>', texte) # Emails texte = re.sub(r'\S+@\S+', '<EMAIL>', texte) # NumÃ©ros de tÃ©lÃ©phone franÃ§ais texte = re.sub(r'0[1-9](?:[0-9]{8})', '<TELEPHONE>', texte) # Dates (format basique) texte = re.sub(r'\d{1,2}/\d{1,2}/\d{4}', '<DATE>', texte) return texte def tokeniser_et_lemmatiser(self, texte): """Tokenisation avec lemmatisation""" doc = self.nlp(texte) tokens = [] for token in doc: if not token.is_space: tokens.append({ 'original': token.text, 'lemme': token.lemma_, 'pos': token.pos_, 'est_alpha': token.is_alpha, 'est_stop': token.is_stop, 'est_punct': token.is_punct }) return tokens def filtrer_tokens(self, tokens): """Filtrage des tokens selon des critÃ¨res""" tokens_filtres = [] for token in tokens: # Garder seulement les mots alphabÃ©tiques if not token['est_alpha']: continue # Supprimer les stop words (optionnel) if token['est_stop']: continue # Supprimer les mots trop courts if len(token['lemme']) < 2: continue tokens_filtres.append(token['lemme']) return tokens_filtres # Utilisation nettoyeur = NettoyeurTexte() texte_sale = """ Salut !!! Comment Ã§a va ??? ğŸ˜Š Mon email: test@exemple.com Site web: https://monsite.fr TÃ©l: 0123456789 On se voit le 15/03/2024 ? """ tokens_propres = nettoyeur.nettoyer_complet(texte_sale) print("Tokens nettoyÃ©s :", tokens_propres) # RÃ©sultat attendu : ['salut', 'aller', 'email', 'site', 'web', 'voir'] ğŸ›ï¸ Options de Preprocessing AvancÃ©es 1. Gestion des NÃ©gations def gerer_negations(texte): """Transforme 'ne ... pas' en 'ne_pas'""" # NÃ©gations franÃ§aises courantes negations = [ (r'\bne\s+(\w+)\s+pas\b', r'ne_\1_pas'), (r'\bn\'(\w+)\s+pas\b', r'ne_\1_pas'), (r'\bne\s+(\w+)\s+jamais\b', r'ne_\1_jamais'), (r'\bne\s+(\w+)\s+plus\b', r'ne_\1_plus'), ] for pattern, replacement in negations: texte = re.sub(pattern, replacement, texte, flags=re.IGNORECASE) return texte # Test phrase = "Je ne suis pas content" phrase_neg = gerer_negations(phrase) print(phrase_neg) # "Je ne_suis_pas content" 2. Expansion des Contractions def expandre_contractions(texte): """Expanse les contractions franÃ§aises""" contractions = { "j'ai": "je ai", "j'Ã©tais": "je Ã©tais", "c'est": "ce est", "c'Ã©tait": "ce Ã©tait", "l'ai": "le ai", "n'ai": "ne ai", "n'est": "ne est", "qu'il": "que il", "qu'elle": "que elle", } for contraction, expansion in contractions.items(): texte = texte.replace(contraction, expansion) return texte 3. Correction Orthographique Basique def corriger_erreurs_courantes(texte): """Corrige les erreurs d'orthographe courantes""" corrections = { "bjr": "bonjour", "bsr": "bonsoir", "slt": "salut", "cc": "coucou", "pk": "pourquoi", "pr": "pour", "ds": "dans", "vs": "vous", "ts": "tous", } for erreur, correction in corrections.items(): texte = re.sub(r'\b' + erreur + r'\b', correction, texte, flags=re.IGNORECASE) return texte ğŸ› ï¸ 04. Outils spaCy et NLTK ğŸ¥Š spaCy vs NLTK : Le Match du SiÃ¨cle CritÃ¨re spaCy NLTK Performance âš¡ TrÃ¨s rapide ğŸŒ Plus lent FacilitÃ© d'usage ğŸ˜Š Simple ğŸ¤” Plus complexe ModÃ¨les prÃ©-entraÃ®nÃ©s âœ… Excellents âš ï¸ Basiques Production âœ… PrÃªt pour prod âš ï¸ PlutÃ´t recherche CommunautÃ© ğŸ”¥ TrÃ¨s active ğŸ“š AcadÃ©mique 

ğŸ¯ Verdict : spaCy pour la production, NLTK pour l'expÃ©rimentation !

ğŸš€ Installation et Setup spaCy # Installation pip install spacy # TÃ©lÃ©chargement du modÃ¨le franÃ§ais python -m spacy download fr_core_news_sm # Pour plus de prÃ©cision (modÃ¨le plus lourd) python -m spacy download fr_core_news_lg ğŸ“‹ spaCy : Guide de DÃ©marrage import spacy # Chargement du modÃ¨le nlp = spacy.load("fr_core_news_sm") def analyser_texte_complet(texte): """Analyse complÃ¨te avec spaCy""" doc = nlp(texte) print("ğŸ” ANALYSE COMPLÃˆTE") print("=" * 50) # 1. Tokenisation de base print("\nğŸ“ TOKENS :") for token in doc: print(f"{token.text:15} | {token.lemma_:15} | {token.pos_:8} | {token.tag_:8}") # 2. EntitÃ©s nommÃ©es print("\nğŸ·ï¸ ENTITÃ‰S NOMMÃ‰ES :") for ent in doc.ents: print(f"{ent.text:20} | {ent.label_:10} | {spacy.explain(ent.label_)}") # 3. Phrases print("\nğŸ“– PHRASES :") for i, sent in enumerate(doc.sents, 1): print(f"Phrase {i}: {sent.text}") # 4. DÃ©pendances syntaxiques (aperÃ§u) print("\nğŸŒ³ DÃ‰PENDANCES (Ã©chantillon) :") for token in doc[:5]: # Premiers 5 tokens seulement print(f"{token.text} â† {token.dep_} â† {token.head.text}") # Test complet texte_test = """ Salut ! Je m'appelle Marie Dupont et je travaille chez Google France. J'habite Ã  Paris depuis 2020. Mon email est marie@google.com. """ analyser_texte_complet(texte_test) ğŸ“š NLTK : Les Incontournables import nltk from nltk.tokenize import word_tokenize, sent_tokenize from nltk.corpus import stopwords from nltk.stem import SnowballStemmer # TÃ©lÃ©chargements nÃ©cessaires (Ã  faire une fois) nltk.download('punkt') nltk.download('stopwords') def analyser_avec_nltk(texte): """Analyse basique avec NLTK""" # Tokenisation par phrases phrases = sent_tokenize(texte, language='french') print(f"ğŸ“– Nombre de phrases : {len(phrases)}") # Tokenisation par mots mots = word_tokenize(texte, language='french') print(f"ğŸ“ Nombre de mots : {len(mots)}") # Stop words franÃ§ais stop_words_fr = set(stopwords.words('french')) mots_filtres = [mot for mot in mots if mot.lower() not in stop_words_fr and mot.isalpha()] print(f"ğŸ” Mots significatifs : {len(mots_filtres)}") # Stemming (racines des mots) stemmer = SnowballStemmer('french') mots_racines = [stemmer.stem(mot) for mot in mots_filtres] print("\nğŸ“Š Ã‰CHANTILLON D'ANALYSE :") for original, racine in zip(mots_filtres[:10], mots_racines[:10]): print(f"{original:15} â†’ {racine}") # Test texte_nltk = "Les dÃ©veloppeurs adorent programmer en Python car c'est un langage fantastique !" analyser_avec_nltk(texte_nltk) ğŸ¯ Comparaison Pratique import time def comparer_performances(texte, nb_iterations=100): """Compare les performances spaCy vs NLTK""" # Test spaCy start_spacy = time.time() for _ in range(nb_iterations): doc = nlp(texte) tokens_spacy = [token.lemma_ for token in doc if token.is_alpha] temps_spacy = time.time() - start_spacy # Test NLTK stemmer = SnowballStemmer('french') stop_words = set(stopwords.words('french')) start_nltk = time.time() for _ in range(nb_iterations): tokens_nltk = word_tokenize(texte, language='french') tokens_nltk = [stemmer.stem(token) for token in tokens_nltk if token.lower() not in stop_words and token.isalpha()] temps_nltk = time.time() - start_nltk print(f"âš¡ spaCy : {temps_spacy:.3f}s | {len(tokens_spacy)} tokens") print(f"ğŸŒ NLTK : {temps_nltk:.3f}s | {len(tokens_nltk)} tokens") print(f"ğŸ“Š Ratio : spaCy est {temps_nltk/temps_spacy:.1f}x plus rapide") # Test de performance texte_perf = "Python est un langage de programmation fantastique pour le machine learning." comparer_performances(texte_perf) ğŸ‹ï¸ Exercices Pratiques ğŸ“ Exercice 1 : Tokenisation NaÃ¯ve vs Intelligente 

ğŸ¯ Objectif : Comprendre les limites de la tokenisation simple

ğŸ“‹ Ã‰noncÃ© :

ImplÃ©mentez un tokenizer naÃ¯f avec split() Testez sur 5 phrases problÃ©matiques fournies Identifiez et listez tous les problÃ¨mes Comparez avec spaCy RÃ©digez un rapport de 200 mots sur vos observations 

ğŸ… CritÃ¨res de rÃ©ussite :

[ ] Code fonctionnel pour les deux approches [ ] Au moins 5 problÃ¨mes identifiÃ©s [ ] Comparaison quantitative (nombre de tokens) [ ] Analyse qualitative dans le rapport ğŸ“ Exercice 2 : Comparaison d'Outils 

ğŸ¯ Objectif : MaÃ®triser spaCy et NLTK

ğŸ“‹ Ã‰noncÃ© :

Tokenisez le mÃªme corpus avec spaCy et NLTK Mesurez les performances (temps d'exÃ©cution) Comparez la qualitÃ© des rÃ©sultats CrÃ©ez un tableau comparatif dÃ©taillÃ© Recommandez un outil selon le contexte 

ğŸ… CritÃ¨res de rÃ©ussite :

[ ] Benchmarks de performance rÃ©alisÃ©s [ ] Tableau comparatif complet [ ] Recommandations justifiÃ©es [ ] Code optimisÃ© et commentÃ© ğŸ“ Exercice 3 : Nettoyage de Tweets 

ğŸ¯ Objectif : CrÃ©er un pipeline de preprocessing robuste

ğŸ“‹ Ã‰noncÃ© :

Dataset fourni : 100 tweets avec URLs, mentions, Ã©mojis CrÃ©ez une classe NettoyeurTweets ImplÃ©mentez 5 Ã©tapes de nettoyage minimum GÃ©nÃ©rez un rapport avant/aprÃ¨s avec statistiques Testez sur des cas edge cases 

ğŸ… CritÃ¨res de rÃ©ussite :

[ ] Pipeline modulaire et rÃ©utilisable [ ] Gestion des cas spÃ©ciaux (Ã©mojis, URLs, etc.) [ ] Rapport statistique dÃ©taillÃ© [ ] Tests sur cas difficiles validÃ©s ğŸ“ Exercice 4 : Debug de Tokenisation 

ğŸ¯ Objectif : DÃ©velopper des compÃ©tences de debugging

ğŸ“‹ Ã‰noncÃ© :

5 textes "cassÃ©s" sont fournis avec des erreurs de tokenisation Identifiez la source de chaque problÃ¨me Proposez une solution pour chaque cas ImplÃ©mentez les corrections Documentez votre approche de debugging 

ğŸ… CritÃ¨res de rÃ©ussite :

[ ] Tous les bugs identifiÃ©s correctement [ ] Solutions Ã©lÃ©gantes implÃ©mentÃ©es [ ] Code dÃ©fensif ajoutÃ© [ ] Documentation du processus de debug ğŸ¯ Projet Final : Explorateur de Texte Interactif ğŸš€ Description du Projet 

CrÃ©ez une application qui analyse n'importe quel texte et rÃ©vÃ¨le ses "secrets linguistiques" !

ğŸ“‹ Cahier des Charges FonctionnalitÃ©s Obligatoires : 

ğŸ“¤ Upload de Fichier

Support : .txt, .pdf, .docx Limite : 10 MB maximum Encodage automatique dÃ©tectÃ© 

ğŸ” Analyse ComplÃ¨te

Statistiques de base (mots, phrases, caractÃ¨res) Distribution des types de mots (noms, verbes, etc.) EntitÃ©s nommÃ©es dÃ©tectÃ©es Mots les plus frÃ©quents (top 10) ComplexitÃ© du texte (longueur moyenne des phrases) 

ğŸ§¹ Pipeline de Nettoyage

Texte original vs texte nettoyÃ© Options configurables de preprocessing Visualisation avant/aprÃ¨s 

ğŸ“Š Visualisations

Nuage de mots Graphique de frÃ©quence des mots Distribution des longueurs de phrases 

ğŸ’¾ Export des RÃ©sultats

JSON avec toutes les analyses CSV des mots avec leurs propriÃ©tÃ©s Rapport PDF gÃ©nÃ©rÃ© automatiquement Interface Utilisateur : # Structure de l'application Streamlit import streamlit as st import spacy import pandas as pd import matplotlib.pyplot as plt from wordcloud import WordCloud st.title("ğŸ” Explorateur de Texte Intelligent") # Sidebar pour les options st.sidebar.header("âš™ï¸ Options de Traitement") supprimer_stopwords = st.sidebar.checkbox("Supprimer les stop words") lemmatiser = st.sidebar.checkbox("Lemmatiser les mots", value=True) min_longueur = st.sidebar.slider("Longueur minimale des mots", 1, 10, 2) # Zone d'upload uploaded_file = st.file_uploader("ğŸ“¤ Choisissez votre fichier", type=['txt', 'pdf', 'docx']) if uploaded_file: # Traitement et affichage des rÃ©sultats pass ğŸ† CritÃ¨res d'Ã‰valuation CritÃ¨re Points Description FonctionnalitÃ©s 40 pts Toutes les fonctions obligatoires Code Quality 20 pts Propre, commentÃ©, modulaire UX/UI 20 pts Interface intuitive et jolie Innovation 10 pts FonctionnalitÃ©s bonus crÃ©atives Documentation 10 pts README.md du projet dÃ©taillÃ© ğŸ Bonus Possibles (+20 points) ğŸŒ Support multilingue : DÃ©tection automatique de la langue ğŸ“ˆ Analyse de sentiment : PolaritÃ© gÃ©nÃ©rale du texte ğŸ”— Extraction d'entitÃ©s : Personnes, lieux, organisations âš¡ Cache intelligent : Ã‰viter de reprocesser les mÃªmes textes ğŸ¨ ThÃ¨mes personnalisables : Interface customisable ğŸ“š Ressources Fournies Code Template # explorateur_texte.py - Structure de base class ExplorateurTexte: def __init__(self): self.nlp = spacy.load("fr_core_news_sm") def charger_fichier(self, fichier): """Charge et lit diffÃ©rents formats de fichiers""" pass def analyser_texte(self, texte, options): """Analyse complÃ¨te du texte""" pass def generer_statistiques(self, doc): """GÃ©nÃ¨re les statistiques descriptives""" stats = { 'nb_mots': len([t for t in doc if t.is_alpha]), 'nb_phrases': len(list(doc.sents)), 'nb_caracteres': len(doc.text), 'mots_uniques': len(set([t.lemma_ for t in doc if t.is_alpha])), 'longueur_moyenne_phrase': None # Ã€ calculer } return stats def extraire_entites(self, doc): """Extrait les entitÃ©s nommÃ©es""" pass def generer_nuage_mots(self, tokens): """CrÃ©e un nuage de mots""" pass Datasets d'Exemple exemple_article.txt : Article de journal franÃ§ais (500 mots) exemple_roman.txt : Extrait de roman (1000 mots) exemple_tweets.txt : Collection de tweets (200 tweets) exemple_technique.txt : Documentation technique (800 mots) Utilitaires Fournis # utils/file_readers.py def lire_pdf(fichier): """Lecture de fichiers PDF""" pass def lire_docx(fichier): """Lecture de fichiers Word""" pass def detecter_encodage(fichier): """DÃ©tection automatique de l'encodage""" pass # utils/visualizations.py def creer_graphique_frequence(mots, frequences): """Graphique en barres des mots frÃ©quents""" pass def creer_nuage_mots(texte): """GÃ©nÃ©ration de nuage de mots stylÃ©""" pass def creer_distribution_longueurs(phrases): """Histogramme des longueurs de phrases""" pass ğŸ“ˆ Progression et Validation âœ… Checklist de Progression Niveau DÃ©butant (Bronze) ğŸ¥‰ [ ] Comprendre pourquoi la tokenisation naÃ¯ve ne suffit pas [ ] Installer et utiliser spaCy correctement [ ] ImplÃ©menter un pipeline de nettoyage basique [ ] RÃ©aliser tous les exercices avec aide [ ] CrÃ©er un explorateur de texte minimal Niveau IntermÃ©diaire (Argent) ğŸ¥ˆ [ ] Expliquer les diffÃ©rences spaCy vs NLTK avec exemples [ ] GÃ©rer les cas spÃ©ciaux (URLs, Ã©mojis, nÃ©gations) [ ] Optimiser les performances de traitement [ ] RÃ©soudre les exercices de faÃ§on autonome [ ] Ajouter des fonctionnalitÃ©s bonus au projet Niveau AvancÃ© (Or) ğŸ¥‡ [ ] CrÃ©er ses propres fonctions de preprocessing [ ] DÃ©boguer et corriger des pipelines cassÃ©s [ ] Proposer des amÃ©liorations aux outils existants [ ] Aider d'autres Ã©tudiants sur les exercices [ ] Projet final avec innovations significatives ğŸ¯ Auto-Ã‰valuation Questions de ComprÃ©hension Conceptuel : Expliquez pourquoi "n'est-ce pas".split() pose problÃ¨me Pratique : Quand utiliser la lemmatisation vs le stemming ? Performance : Pourquoi spaCy est-il plus rapide que NLTK ? Architecture : Comment structurer un pipeline de preprocessing rÃ©utilisable ? DÃ©fis de Code # DÃ©fi 1 : Tokenisation robuste def tokeniser_robuste(texte): """ CrÃ©ez un tokenizer qui gÃ¨re : - Les contractions franÃ§aises - Les URLs et emails - Les Ã©mojis - Les nÃ©gations """ pass # DÃ©fi 2 : DÃ©tection d'anomalies def detecter_anomalies_texte(texte): """ Identifiez automatiquement : - Encodage incorrect - Texte gÃ©nÃ©rÃ© par IA - Langue incorrecte - Formatting cassÃ© """ pass ğŸ”— Liens et Ressources ComplÃ©mentaires ğŸ“– Documentation Officielle spaCy Documentation - Guide complet NLTK Book - RÃ©fÃ©rence acadÃ©mique Regex101 - Testeur d'expressions rÃ©guliÃ¨res ğŸ¥ VidÃ©os RecommandÃ©es "spaCy IRL" (YouTube) - Cas d'usage rÃ©els "Text Preprocessing Explained" - Concepts visuels "French NLP Challenges" - SpÃ©cificitÃ©s du franÃ§ais ğŸ“š Articles AvancÃ©s "Why Tokenization Matters" - Importance en NLP "French Language Processing" - DÃ©fis spÃ©cifiques "Production NLP Pipelines" - Bonnes pratiques ğŸ› ï¸ Outils ComplÃ©mentaires Stanza : Alternative Ã  spaCy (Stanford) TextBlob : SimplicitÃ© maximale Polyglot : Support multilingue Ã©tendu ğŸš€ PrÃ©paration au Module 2 ğŸ¯ Ce que vous avez acquis âœ… MaÃ®trise de la tokenisation intelligente âœ… Pipelines de preprocessing robustes âœ… Utilisation experte de spaCy et NLTK âœ… Debugging de problÃ¨mes textuels âœ… Application complÃ¨te fonctionnelle ğŸ”® Ce qui vous attend ğŸ”¢ Vectorisation : Transformer vos tokens en nombres ğŸ“Š TF-IDF : Mesurer l'importance des mots ğŸ§  Word Embeddings : Capturer le sens sÃ©mantique ğŸ¯ SimilaritÃ© : Comparer des textes automatiquement ğŸ’¡ Conseil pour la Suite 

"Maintenant que vous savez 'dÃ©couper' le langage, vous allez apprendre Ã  le 'mesurer' ! Les tokens que vous crÃ©ez ici vont devenir les coordonnÃ©es GPS de vos mots dans l'espace mathÃ©matique. Gardez vos pipelines de preprocessing : vous allez en avoir besoin !"

ğŸ“ Support et CommunautÃ© ğŸ¤ Aide Entre Ã‰tudiants GitHub Discussions : Posez vos questions techniques Discord NLP-France : Chat en temps rÃ©el Peer Review : Ã‰changez vos codes et solutions ğŸ†˜ En Cas de Blocage Consultez la FAQ des erreurs courantes Utilisez le debugger intÃ©grÃ© des notebooks Postez votre code avec le message d'erreur exact Demandez une review de votre approche ğŸ† Partager ses RÃ©ussites Portfolio GitHub : Montrez vos projets LinkedIn : Partagez vos accomplissements Blog technique : Expliquez vos apprentissages ğŸ“ Notes Finales 

ğŸ‰ FÃ©licitations ! Si vous Ãªtes arrivÃ© jusqu'ici, vous maÃ®trisez maintenant les fondations du NLP. Vous savez transformer du texte "sale" en donnÃ©es exploitables par les machines.

ğŸ”¥ Point ClÃ© : 80% du travail en NLP, c'est le preprocessing ! Vous venez d'acquÃ©rir une compÃ©tence absolument cruciale.

ğŸš€ Next Level : Dans le module 2, nous allons transformer vos beaux tokens en vecteurs mathÃ©matiques. C'est lÃ  que la vraie magie commence !

DerniÃ¨re mise Ã  jour : [Date] | Version 1.0 Contributeurs : [Votre nom] | Retours : [email/discord]

