# üìù Module 1 : Anatomie du Texte
## *Apprendre √† d√©couper le langage humain pour les machines*

---

## üé¨ **Vid√©o d'Introduction** *(5 minutes)*

### üì∫ **[Regarder la Vid√©o Intro](video-intro.md)**

**üí° R√©sum√© de la vid√©o :**
> "D√©couvrez pourquoi Siri vous comprend parfois si mal ! Dans ce module, nous allons d√©cortiquer le texte comme un chirurgien pour comprendre comment transformer des mots en quelque chose que les machines peuvent dig√©rer. Vous allez apprendre √† 'd√©couper' intelligemment le langage humain et nettoyer vos donn√©es textuelles comme un pro. √Ä la fin, vous aurez cr√©√© votre propre explorateur de texte qui r√©v√®le tous les secrets cach√©s dans n'importe quel document !"

---

## üéØ **Objectifs d'Apprentissage**

√Ä la fin de ce module, vous serez capable de :

- ‚úÖ **Comprendre** pourquoi les machines ont du mal avec le langage humain
- ‚úÖ **Ma√Ætriser** la tokenisation et ses d√©fis
- ‚úÖ **Impl√©menter** des pipelines de preprocessing robustes
- ‚úÖ **Utiliser** spaCy et NLTK efficacement
- ‚úÖ **D√©boguer** les probl√®mes courants de traitement de texte
- ‚úÖ **Cr√©er** un explorateur de texte interactif

---

## üìö **Plan du Module**

| Section | Contenu | Dur√©e | Difficult√© |
|---------|---------|-------|------------|
| [01](#01-introduction) | Introduction au Probl√®me | 45 min | ‚≠ê‚òÜ‚òÜ |
| [02](#02-tokenisation) | Tokenisation Intelligente | 60 min | ‚≠ê‚≠ê‚òÜ |
| [03](#03-preprocessing) | Preprocessing et Nettoyage | 60 min | ‚≠ê‚≠ê‚òÜ |
| [04](#04-outils) | Outils spaCy et NLTK | 45 min | ‚≠ê‚≠ê‚òÜ |
| [Exercices](#exercices) | 4 Exercices Pratiques | 120 min | ‚≠ê‚≠ê‚≠ê |
| [Projet](#projet-final) | Explorateur de Texte | 90 min | ‚≠ê‚≠ê‚≠ê |

**‚è±Ô∏è Dur√©e totale estim√©e : 6-7 heures**

---

## üìñ **01. Introduction au Probl√®me**

### ü§î **Pourquoi les Machines D√©testent le Langage Humain ?**

Imaginez que vous devez expliquer √† un extraterrestre ce que signifie "Il fait un temps de chien" quand il pleut. Pas √©vident, non ? C'est exactement le probl√®me des machines avec notre langage !

#### **Le Foss√© Texte ‚Üî Nombres**

```python
# Ce que nous voyons
texte_humain = "Salut ! Comment √ßa va ? üòä"

# Ce que voit l'ordinateur (repr√©sentation ASCII)
[83, 97, 108, 117, 116, 32, 33, 32, 67, 111, 109, 109, 101, 110, 116, ...]
```

**üéØ Probl√®mes majeurs :**

1. **Ambigu√Øt√©** : "La poule du pot" (repas vs r√©cipient ?)
2. **Contexte** : "Il est cool" vs "Il fait cool"
3. **Variations** : "super", "g√©nial", "fantastique" = m√™me sens
4. **Erreurs** : "bjr", "slt", "cc" = variations informelles
5. **Multilingue** : m√©langes fran√ßais/anglais/argot

#### **üß™ Exp√©rience : Tester Google Translate**

Essayez de traduire ces phrases et observez les erreurs :

```
1. "Je suis dans le rouge ce mois-ci"
2. "Il a pris la mouche"
3. "C'est du chinois pour moi"
4. "Elle a un poil dans la main"
```

**üí° Analyse :** Google Translate traduit litt√©ralement car il ne comprend pas les expressions idiomatiques !

### üéØ **Notre Mission**

Transformer ce chaos linguistique en donn√©es exploitables par les machines. C'est le pr√©alable OBLIGATOIRE √† toute application NLP !

---

## üî™ **02. Tokenisation Intelligente**

### üìö **Qu'est-ce que la Tokenisation ?**

**D√©finition :** D√©couper un texte en unit√©s plus petites (tokens) : mots, phrases, caract√®res, etc.

**Analogie :** Comme d√©couper les ingr√©dients avant de cuisiner ! üë®‚Äçüç≥

### üö´ **L'Approche Na√Øve (qui ne marche pas)**

```python
# M√©thode d√©butant (MAUVAISE)
texte = "Bonjour, comment allez-vous ? J'esp√®re que √ßa va !"
tokens_naifs = texte.split(" ")
print(tokens_naifs)
# R√©sultat : ['Bonjour,', 'comment', 'allez-vous', '?', "J'esp√®re", ...]
```

**üî• Probl√®mes identifi√©s :**
- Ponctuation coll√©e aux mots
- Contractions mal g√©r√©es
- Majuscules conserv√©es
- Espaces multiples ignor√©s

### ‚úÖ **L'Approche Intelligente**

```python
import spacy

# Chargement du mod√®le fran√ßais
nlp = spacy.load("fr_core_news_sm")

def tokeniser_intelligemment(texte):
    """Tokenise un texte avec spaCy"""
    doc = nlp(texte)
    
    tokens = []
    for token in doc:
        if not token.is_space:  # Ignorer les espaces
            tokens.append({
                'texte': token.text,
                'lemme': token.lemma_,
                'pos': token.pos_,
                'est_ponctuation': token.is_punct,
                'est_stop_word': token.is_stop
            })
    
    return tokens

# Test
texte = "Bonjour, comment allez-vous ? J'esp√®re que √ßa va !"
tokens = tokeniser_intelligemment(texte)

for token in tokens:
    print(f"{token['texte']:15} | {token['lemme']:15} | {token['pos']}")
```

**üìä R√©sultat attendu :**
```
Bonjour         | bonjour         | INTJ
,               | ,               | PUNCT
comment         | comment         | ADV
allez           | aller           | VERB
-               | -               | PUNCT
vous            | vous            | PRON
?               | ?               | PUNCT
J'              | je              | PRON
esp√®re          | esp√©rer         | VERB
que             | que             | SCONJ
√ßa              | √ßa              | PRON
va              | aller           | VERB
!               | !               | PUNCT
```

### üéØ **Types de Tokenisation**

#### **1. Tokenisation par Mots**
```python
# Standard pour la plupart des applications
doc = nlp("Python est g√©nial pour le NLP !")
mots = [token.text for token in doc if not token.is_punct]
# R√©sultat : ['Python', 'est', 'g√©nial', 'pour', 'le', 'NLP']
```

#### **2. Tokenisation par Phrases**
```python
# Utile pour l'analyse de documents longs
texte = "Python est super. J'adore ce langage ! Et vous ?"
doc = nlp(texte)
phrases = [sent.text for sent in doc.sents]
# R√©sultat : ['Python est super.', "J'adore ce langage !", 'Et vous ?']
```

#### **3. Tokenisation par Caract√®res**
```python
# Pour les langues sans espaces (chinois) ou l'analyse fine
mot = "g√©nial"
caracteres = list(mot)
# R√©sultat : ['g', '√©', 'n', 'i', 'a', 'l']
```

### üîß **Gestion des Cas Sp√©ciaux**

#### **URLs et Mentions**
```python
import re

def nettoyer_urls_mentions(texte):
    """Remplace URLs et mentions par des tokens sp√©ciaux"""
    # URLs
    texte = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 
                   '<URL>', texte)
    
    # Mentions Twitter
    texte = re.sub(r'@\w+', '<MENTION>', texte)
    
    # Hashtags
    texte = re.sub(r'#\w+', '<HASHTAG>', texte)
    
    return texte

# Test
tweet = "Regardez cette vid√©o https://youtube.com/watch?v=123 @elonmusk #IA #cool"
tweet_propre = nettoyer_urls_mentions(tweet)
print(tweet_propre)
# R√©sultat : "Regardez cette vid√©o <URL> <MENTION> <HASHTAG> <HASHTAG>"
```

#### **√âmojis et Caract√®res Sp√©ciaux**
```python
import emoji

def gerer_emojis(texte):
    """Convertit les √©mojis en texte descriptif"""
    return emoji.demojize(texte, language='fr')

# Test
texte_emoji = "J'adore Python ! üòçüêç"
texte_sans_emoji = gerer_emojis(texte_emoji)
print(texte_sans_emoji)
# R√©sultat : "J'adore Python ! :visage_souriant_avec_des_yeux_en_forme_de_c≈ìur::serpent:"
```

---

## üßπ **03. Preprocessing et Nettoyage**

### üéØ **Objectif du Preprocessing**

Transformer un texte "sale" en texte "propre" et standardis√© pour l'analyse.

**Principe :** Plus vos donn√©es sont propres, meilleurs seront vos r√©sultats !

### üîß **Pipeline de Nettoyage Standard**

```python
import re
import string
from unidecode import unidecode

class NettoyeurTexte:
    def __init__(self):
        self.nlp = spacy.load("fr_core_news_sm")
    
    def nettoyer_complet(self, texte):
        """Pipeline complet de nettoyage"""
        # √âtape 1 : Normalisation de base
        texte = self.normaliser_base(texte)
        
        # √âtape 2 : Gestion des entit√©s sp√©ciales
        texte = self.gerer_entites_speciales(texte)
        
        # √âtape 3 : Tokenisation et lemmatisation
        tokens = self.tokeniser_et_lemmatiser(texte)
        
        # √âtape 4 : Filtrage
        tokens = self.filtrer_tokens(tokens)
        
        return tokens
    
    def normaliser_base(self, texte):
        """Normalisation basique du texte"""
        # Conversion en minuscules
        texte = texte.lower()
        
        # Suppression des accents (optionnel)
        # texte = unidecode(texte)
        
        # Suppression des caract√®res de contr√¥le
        texte = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', texte)
        
        # Normalisation des espaces
        texte = re.sub(r'\s+', ' ', texte)
        
        return texte.strip()
    
    def gerer_entites_speciales(self, texte):
        """Gestion des URLs, emails, etc."""
        # URLs
        texte = re.sub(r'http[s]?://\S+', '<URL>', texte)
        
        # Emails
        texte = re.sub(r'\S+@\S+', '<EMAIL>', texte)
        
        # Num√©ros de t√©l√©phone fran√ßais
        texte = re.sub(r'0[1-9](?:[0-9]{8})', '<TELEPHONE>', texte)
        
        # Dates (format basique)
        texte = re.sub(r'\d{1,2}/\d{1,2}/\d{4}', '<DATE>', texte)
        
        return texte
    
    def tokeniser_et_lemmatiser(self, texte):
        """Tokenisation avec lemmatisation"""
        doc = self.nlp(texte)
        
        tokens = []
        for token in doc:
            if not token.is_space:
                tokens.append({
                    'original': token.text,
                    'lemme': token.lemma_,
                    'pos': token.pos_,
                    'est_alpha': token.is_alpha,
                    'est_stop': token.is_stop,
                    'est_punct': token.is_punct
                })
        
        return tokens
    
    def filtrer_tokens(self, tokens):
        """Filtrage des tokens selon des crit√®res"""
        tokens_filtres = []
        
        for token in tokens:
            # Garder seulement les mots alphab√©tiques
            if not token['est_alpha']:
                continue
            
            # Supprimer les stop words (optionnel)
            if token['est_stop']:
                continue
            
            # Supprimer les mots trop courts
            if len(token['lemme']) < 2:
                continue
            
            tokens_filtres.append(token['lemme'])
        
        return tokens_filtres

# Utilisation
nettoyeur = NettoyeurTexte()

texte_sale = """
Salut !!! Comment √ßa va ??? üòä 
Mon email: test@exemple.com
Site web: https://monsite.fr
T√©l: 0123456789
On se voit le 15/03/2024 ?
"""

tokens_propres = nettoyeur.nettoyer_complet(texte_sale)
print("Tokens nettoy√©s :", tokens_propres)
# R√©sultat attendu : ['salut', 'aller', 'email', 'site', 'web', 'voir']
```

### üéõÔ∏è **Options de Preprocessing Avanc√©es**

#### **1. Gestion des N√©gations**
```python
def gerer_negations(texte):
    """Transforme 'ne ... pas' en 'ne_pas'"""
    # N√©gations fran√ßaises courantes
    negations = [
        (r'\bne\s+(\w+)\s+pas\b', r'ne_\1_pas'),
        (r'\bn\'(\w+)\s+pas\b', r'ne_\1_pas'),
        (r'\bne\s+(\w+)\s+jamais\b', r'ne_\1_jamais'),
        (r'\bne\s+(\w+)\s+plus\b', r'ne_\1_plus'),
    ]
    
    for pattern, replacement in negations:
        texte = re.sub(pattern, replacement, texte, flags=re.IGNORECASE)
    
    return texte

# Test
phrase = "Je ne suis pas content"
phrase_neg = gerer_negations(phrase)
print(phrase_neg)  # "Je ne_suis_pas content"
```

#### **2. Expansion des Contractions**
```python
def expandre_contractions(texte):
    """Expanse les contractions fran√ßaises"""
    contractions = {
        "j'ai": "je ai",
        "j'√©tais": "je √©tais",
        "c'est": "ce est",
        "c'√©tait": "ce √©tait",
        "l'ai": "le ai",
        "n'ai": "ne ai",
        "n'est": "ne est",
        "qu'il": "que il",
        "qu'elle": "que elle",
    }
    
    for contraction, expansion in contractions.items():
        texte = texte.replace(contraction, expansion)
    
    return texte
```

#### **3. Correction Orthographique Basique**
```python
def corriger_erreurs_courantes(texte):
    """Corrige les erreurs d'orthographe courantes"""
    corrections = {
        "bjr": "bonjour",
        "bsr": "bonsoir",
        "slt": "salut",
        "cc": "coucou",
        "pk": "pourquoi",
        "pr": "pour",
        "ds": "dans",
        "vs": "vous",
        "ts": "tous",
    }
    
    for erreur, correction in corrections.items():
        texte = re.sub(r'\b' + erreur + r'\b', correction, texte, flags=re.IGNORECASE)
    
    return texte
```

---

## üõ†Ô∏è **04. Outils spaCy et NLTK**

### ü•ä **spaCy vs NLTK : Le Match du Si√®cle**

| Crit√®re | spaCy | NLTK |
|---------|-------|------|
| **Performance** | ‚ö° Tr√®s rapide | üêå Plus lent |
| **Facilit√© d'usage** | üòä Simple | ü§î Plus complexe |
| **Mod√®les pr√©-entra√Æn√©s** | ‚úÖ Excellents | ‚ö†Ô∏è Basiques |
| **Production** | ‚úÖ Pr√™t pour prod | ‚ö†Ô∏è Plut√¥t recherche |
| **Communaut√©** | üî• Tr√®s active | üìö Acad√©mique |

**üéØ Verdict :** spaCy pour la production, NLTK pour l'exp√©rimentation !

### üöÄ **Installation et Setup spaCy**

```bash
# Installation
pip install spacy

# T√©l√©chargement du mod√®le fran√ßais
python -m spacy download fr_core_news_sm

# Pour plus de pr√©cision (mod√®le plus lourd)
python -m spacy download fr_core_news_lg
```

### üìã **spaCy : Guide de D√©marrage**

```python
import spacy

# Chargement du mod√®le
nlp = spacy.load("fr_core_news_sm")

def analyser_texte_complet(texte):
    """Analyse compl√®te avec spaCy"""
    doc = nlp(texte)
    
    print("üîç ANALYSE COMPL√àTE")
    print("=" * 50)
    
    # 1. Tokenisation de base
    print("\nüìù TOKENS :")
    for token in doc:
        print(f"{token.text:15} | {token.lemma_:15} | {token.pos_:8} | {token.tag_:8}")
    
    # 2. Entit√©s nomm√©es
    print("\nüè∑Ô∏è ENTIT√âS NOMM√âES :")
    for ent in doc.ents:
        print(f"{ent.text:20} | {ent.label_:10} | {spacy.explain(ent.label_)}")
    
    # 3. Phrases
    print("\nüìñ PHRASES :")
    for i, sent in enumerate(doc.sents, 1):
        print(f"Phrase {i}: {sent.text}")
    
    # 4. D√©pendances syntaxiques (aper√ßu)
    print("\nüå≥ D√âPENDANCES (√©chantillon) :")
    for token in doc[:5]:  # Premiers 5 tokens seulement
        print(f"{token.text} ‚Üê {token.dep_} ‚Üê {token.head.text}")

# Test complet
texte_test = """
Salut ! Je m'appelle Marie Dupont et je travaille chez Google France.
J'habite √† Paris depuis 2020. Mon email est marie@google.com.
"""

analyser_texte_complet(texte_test)
```

### üìö **NLTK : Les Incontournables**

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

# T√©l√©chargements n√©cessaires (√† faire une fois)
nltk.download('punkt')
nltk.download('stopwords')

def analyser_avec_nltk(texte):
    """Analyse basique avec NLTK"""
    
    # Tokenisation par phrases
    phrases = sent_tokenize(texte, language='french')
    print(f"üìñ Nombre de phrases : {len(phrases)}")
    
    # Tokenisation par mots
    mots = word_tokenize(texte, language='french')
    print(f"üìù Nombre de mots : {len(mots)}")
    
    # Stop words fran√ßais
    stop_words_fr = set(stopwords.words('french'))
    mots_filtres = [mot for mot in mots if mot.lower() not in stop_words_fr and mot.isalpha()]
    print(f"üîç Mots significatifs : {len(mots_filtres)}")
    
    # Stemming (racines des mots)
    stemmer = SnowballStemmer('french')
    mots_racines = [stemmer.stem(mot) for mot in mots_filtres]
    
    print("\nüìä √âCHANTILLON D'ANALYSE :")
    for original, racine in zip(mots_filtres[:10], mots_racines[:10]):
        print(f"{original:15} ‚Üí {racine}")

# Test
texte_nltk = "Les d√©veloppeurs adorent programmer en Python car c'est un langage fantastique !"
analyser_avec_nltk(texte_nltk)
```

### üéØ **Comparaison Pratique**

```python
import time

def comparer_performances(texte, nb_iterations=100):
    """Compare les performances spaCy vs NLTK"""
    
    # Test spaCy
    start_spacy = time.time()
    for _ in range(nb_iterations):
        doc = nlp(texte)
        tokens_spacy = [token.lemma_ for token in doc if token.is_alpha]
    temps_spacy = time.time() - start_spacy
    
    # Test NLTK
    stemmer = SnowballStemmer('french')
    stop_words = set(stopwords.words('french'))
    
    start_nltk = time.time()
    for _ in range(nb_iterations):
        tokens_nltk = word_tokenize(texte, language='french')
        tokens_nltk = [stemmer.stem(token) for token in tokens_nltk 
                      if token.lower() not in stop_words and token.isalpha()]
    temps_nltk = time.time() - start_nltk
    
    print(f"‚ö° spaCy  : {temps_spacy:.3f}s | {len(tokens_spacy)} tokens")
    print(f"üêå NLTK   : {temps_nltk:.3f}s | {len(tokens_nltk)} tokens")
    print(f"üìä Ratio  : spaCy est {temps_nltk/temps_spacy:.1f}x plus rapide")

# Test de performance
texte_perf = "Python est un langage de programmation fantastique pour le machine learning."
comparer_performances(texte_perf)
```

---

## üèãÔ∏è **Exercices Pratiques**

### üìù **Exercice 1 : Tokenisation Na√Øve vs Intelligente**
**üéØ Objectif :** Comprendre les limites de la tokenisation simple

**üìã √ânonc√© :**
1. Impl√©mentez un tokenizer na√Øf avec `split()`
2. Testez sur 5 phrases probl√©matiques fournies
3. Identifiez et listez tous les probl√®mes
4. Comparez avec spaCy
5. R√©digez un rapport de 200 mots sur vos observations

**üèÖ Crit√®res de r√©ussite :**
- [ ] Code fonctionnel pour les deux approches
- [ ] Au moins 5 probl√®mes identifi√©s
- [ ] Comparaison quantitative (nombre de tokens)
- [ ] Analyse qualitative dans le rapport

---

### üìù **Exercice 2 : Comparaison d'Outils**
**üéØ Objectif :** Ma√Ætriser spaCy et NLTK

**üìã √ânonc√© :**
1. Tokenisez le m√™me corpus avec spaCy et NLTK
2. Mesurez les performances (temps d'ex√©cution)
3. Comparez la qualit√© des r√©sultats
4. Cr√©ez un tableau comparatif d√©taill√©
5. Recommandez un outil selon le contexte

**üèÖ Crit√®res de r√©ussite :**
- [ ] Benchmarks de performance r√©alis√©s
- [ ] Tableau comparatif complet
- [ ] Recommandations justifi√©es
- [ ] Code optimis√© et comment√©

---

### üìù **Exercice 3 : Nettoyage de Tweets**
**üéØ Objectif :** Cr√©er un pipeline de preprocessing robuste

**üìã √ânonc√© :**
1. Dataset fourni : 100 tweets avec URLs, mentions, √©mojis
2. Cr√©ez une classe `NettoyeurTweets`
3. Impl√©mentez 5 √©tapes de nettoyage minimum
4. G√©n√©rez un rapport avant/apr√®s avec statistiques
5. Testez sur des cas edge cases

**üèÖ Crit√®res de r√©ussite :**
- [ ] Pipeline modulaire et r√©utilisable
- [ ] Gestion des cas sp√©ciaux (√©mojis, URLs, etc.)
- [ ] Rapport statistique d√©taill√©
- [ ] Tests sur cas difficiles valid√©s

---

### üìù **Exercice 4 : Debug de Tokenisation**
**üéØ Objectif :** D√©velopper des comp√©tences de debugging

**üìã √ânonc√© :**
1. 5 textes "cass√©s" sont fournis avec des erreurs de tokenisation
2. Identifiez la source de chaque probl√®me
3. Proposez une solution pour chaque cas
4. Impl√©mentez les corrections
5. Documentez votre approche de debugging

**üèÖ Crit√®res de r√©ussite :**
- [ ] Tous les bugs identifi√©s correctement
- [ ] Solutions √©l√©gantes impl√©ment√©es
- [ ] Code d√©fensif ajout√©
- [ ] Documentation du processus de debug

---

## üéØ **Projet Final : Explorateur de Texte Interactif**

### üöÄ **Description du Projet**

Cr√©ez une application qui analyse n'importe quel texte et r√©v√®le ses "secrets linguistiques" !

### üìã **Cahier des Charges**

#### **Fonctionnalit√©s Obligatoires :**

1. **üì§ Upload de Fichier**
   - Support : .txt, .pdf, .docx
   - Limite : 10 MB maximum
   - Encodage automatique d√©tect√©

2. **üîç Analyse Compl√®te**
   - Statistiques de base (mots, phrases, caract√®res)
   - Distribution des types de mots (noms, verbes, etc.)
   - Entit√©s nomm√©es d√©tect√©es
   - Mots les plus fr√©quents (top 10)
   - Complexit√© du texte (longueur moyenne des phrases)

3. **üßπ Pipeline de Nettoyage**
   - Texte original vs texte nettoy√©
   - Options configurables de preprocessing
   - Visualisation avant/apr√®s

4. **üìä Visualisations**
   - Nuage de mots
   - Graphique de fr√©quence des mots
   - Distribution des longueurs de phrases

5. **üíæ Export des R√©sultats**
   - JSON avec toutes les analyses
   - CSV des mots avec leurs propri√©t√©s
   - Rapport PDF g√©n√©r√© automatiquement

#### **Interface Utilisateur :**

```python
# Structure de l'application Streamlit
import streamlit as st
import spacy
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

st.title("üîç Explorateur de Texte Intelligent")

# Sidebar pour les options
st.sidebar.header("‚öôÔ∏è Options de Traitement")
supprimer_stopwords = st.sidebar.checkbox("Supprimer les stop words")
lemmatiser = st.sidebar.checkbox("Lemmatiser les mots", value=True)
min_longueur = st.sidebar.slider("Longueur minimale des mots", 1, 10, 2)

# Zone d'upload
uploaded_file = st.file_uploader("üì§ Choisissez votre fichier", 
                                 type=['txt', 'pdf', 'docx'])

if uploaded_file:
    # Traitement et affichage des r√©sultats
    pass
```

### üèÜ **Crit√®res d'√âvaluation**

| Crit√®re | Points | Description |
|---------|--------|-------------|
| **Fonctionnalit√©s** | 40 pts | Toutes les fonctions obligatoires |
| **Code Quality** | 20 pts | Propre, comment√©, modulaire |
| **UX/UI** | 20 pts | Interface intuitive et jolie |
| **Innovation** | 10 pts | Fonctionnalit√©s bonus cr√©atives |
| **Documentation** | 10 pts | README.md du projet d√©taill√© |

### üéÅ **Bonus Possibles (+20 points)**

- **üåê Support multilingue** : D√©tection automatique de la langue
- **üìà Analyse de sentiment** : Polarit√© g√©n√©rale du texte
- **üîó Extraction d'entit√©s** : Personnes, lieux, organisations
- **‚ö° Cache intelligent** : √âviter de reprocesser les m√™mes textes
- **üé® Th√®mes personnalisables** : Interface customisable

### üìö **Ressources Fournies**

#### **Code Template**
```python
# explorateur_texte.py - Structure de base
class ExplorateurTexte:
    def __init__(self):
        self.nlp = spacy.load("fr_core_news_sm")
        
    def charger_fichier(self, fichier):
        """Charge et lit diff√©rents formats de fichiers"""
        pass
    
    def analyser_texte(self, texte, options):
        """Analyse compl√®te du texte"""
        pass
    
    def generer_statistiques(self, doc):
        """G√©n√®re les statistiques descriptives"""
        stats = {
            'nb_mots': len([t for t in doc if t.is_alpha]),
            'nb_phrases': len(list(doc.sents)),
            'nb_caracteres': len(doc.text),
            'mots_uniques': len(set([t.lemma_ for t in doc if t.is_alpha])),
            'longueur_moyenne_phrase': None  # √Ä calculer
        }
        return stats
    
    def extraire_entites(self, doc):
        """Extrait les entit√©s nomm√©es"""
        pass
    
    def generer_nuage_mots(self, tokens):
        """Cr√©e un nuage de mots"""
        pass
```

#### **Datasets d'Exemple**
- `exemple_article.txt` : Article de journal fran√ßais (500 mots)
- `exemple_roman.txt` : Extrait de roman (1000 mots)  
- `exemple_tweets.txt` : Collection de tweets (200 tweets)
- `exemple_technique.txt` : Documentation technique (800 mots)

#### **Utilitaires Fournis**
```python
# utils/file_readers.py
def lire_pdf(fichier):
    """Lecture de fichiers PDF"""
    pass

def lire_docx(fichier):
    """Lecture de fichiers Word"""
    pass

def detecter_encodage(fichier):
    """D√©tection automatique de l'encodage"""
    pass

# utils/visualizations.py
def creer_graphique_frequence(mots, frequences):
    """Graphique en barres des mots fr√©quents"""
    pass

def creer_nuage_mots(texte):
    """G√©n√©ration de nuage de mots styl√©"""
    pass

def creer_distribution_longueurs(phrases):
    """Histogramme des longueurs de phrases"""
    pass
```

---

## üìà **Progression et Validation**

### ‚úÖ **Checklist de Progression**

#### **Niveau D√©butant (Bronze)** ü•â
- [ ] Comprendre pourquoi la tokenisation na√Øve ne suffit pas
- [ ] Installer et utiliser spaCy correctement
- [ ] Impl√©menter un pipeline de nettoyage basique
- [ ] R√©aliser tous les exercices avec aide
- [ ] Cr√©er un explorateur de texte minimal

#### **Niveau Interm√©diaire (Argent)** ü•à
- [ ] Expliquer les diff√©rences spaCy vs NLTK avec exemples
- [ ] G√©rer les cas sp√©ciaux (URLs, √©mojis, n√©gations)
- [ ] Optimiser les performances de traitement
- [ ] R√©soudre les exercices de fa√ßon autonome
- [ ] Ajouter des fonctionnalit√©s bonus au projet

#### **Niveau Avanc√© (Or)** ü•á
- [ ] Cr√©er ses propres fonctions de preprocessing
- [ ] D√©boguer et corriger des pipelines cass√©s
- [ ] Proposer des am√©liorations aux outils existants
- [ ] Aider d'autres √©tudiants sur les exercices
- [ ] Projet final avec innovations significatives

### üéØ **Auto-√âvaluation**

#### **Questions de Compr√©hension**

1. **Conceptuel** : Expliquez pourquoi `"n'est-ce pas".split()` pose probl√®me
2. **Pratique** : Quand utiliser la lemmatisation vs le stemming ?
3. **Performance** : Pourquoi spaCy est-il plus rapide que NLTK ?
4. **Architecture** : Comment structurer un pipeline de preprocessing r√©utilisable ?

#### **D√©fis de Code**

```python
# D√©fi 1 : Tokenisation robuste
def tokeniser_robuste(texte):
    """
    Cr√©ez un tokenizer qui g√®re :
    - Les contractions fran√ßaises
    - Les URLs et emails
    - Les √©mojis
    - Les n√©gations
    """
    pass

# D√©fi 2 : D√©tection d'anomalies
def detecter_anomalies_texte(texte):
    """
    Identifiez automatiquement :
    - Encodage incorrect
    - Texte g√©n√©r√© par IA
    - Langue incorrecte
    - Formatting cass√©
    """
    pass
```

---

## üîó **Liens et Ressources Compl√©mentaires**

### üìñ **Documentation Officielle**
- [spaCy Documentation](https://spacy.io/usage/spacy-101) - Guide complet
- [NLTK Book](https://www.nltk.org/book/) - R√©f√©rence acad√©mique
- [Regex101](https://regex101.com/) - Testeur d'expressions r√©guli√®res

### üé• **Vid√©os Recommand√©es**
- "spaCy IRL" (YouTube) - Cas d'usage r√©els
- "Text Preprocessing Explained" - Concepts visuels
- "French NLP Challenges" - Sp√©cificit√©s du fran√ßais

### üìö **Articles Avanc√©s**
- "Why Tokenization Matters" - Importance en NLP
- "French Language Processing" - D√©fis sp√©cifiques
- "Production NLP Pipelines" - Bonnes pratiques

### üõ†Ô∏è **Outils Compl√©mentaires**
- **Stanza** : Alternative √† spaCy (Stanford)
- **TextBlob** : Simplicit√© maximale
- **Polyglot** : Support multilingue √©tendu

---

## üöÄ **Pr√©paration au Module 2**

### üéØ **Ce que vous avez acquis**
- ‚úÖ Ma√Ætrise de la tokenisation intelligente
- ‚úÖ Pipelines de preprocessing robustes  
- ‚úÖ Utilisation experte de spaCy et NLTK
- ‚úÖ Debugging de probl√®mes textuels
- ‚úÖ Application compl√®te fonctionnelle

### üîÆ **Ce qui vous attend**
- üî¢ **Vectorisation** : Transformer vos tokens en nombres
- üìä **TF-IDF** : Mesurer l'importance des mots
- üß† **Word Embeddings** : Capturer le sens s√©mantique
- üéØ **Similarit√©** : Comparer des textes automatiquement

### üí° **Conseil pour la Suite**
> "Maintenant que vous savez 'd√©couper' le langage, vous allez apprendre √† le 'mesurer' ! Les tokens que vous cr√©ez ici vont devenir les coordonn√©es GPS de vos mots dans l'espace math√©matique. Gardez vos pipelines de preprocessing : vous allez en avoir besoin !"

---

## üìû **Support et Communaut√©**

### ü§ù **Aide Entre √âtudiants**
- **GitHub Discussions** : Posez vos questions techniques
- **Discord NLP-France** : Chat en temps r√©el  
- **Peer Review** : √âchangez vos codes et solutions

### üÜò **En Cas de Blocage**
1. **Consultez la FAQ** des erreurs courantes
2. **Utilisez le debugger** int√©gr√© des notebooks
3. **Postez votre code** avec le message d'erreur exact
4. **Demandez une review** de votre approche

### üèÜ **Partager ses R√©ussites**
- **Portfolio GitHub** : Montrez vos projets
- **LinkedIn** : Partagez vos accomplissements
- **Blog technique** : Expliquez vos apprentissages

---

## üìù **Notes Finales**

**üéâ F√©licitations !** Si vous √™tes arriv√© jusqu'ici, vous ma√Ætrisez maintenant les fondations du NLP. Vous savez transformer du texte "sale" en donn√©es exploitables par les machines.

**üî• Point Cl√© :** 80% du travail en NLP, c'est le preprocessing ! Vous venez d'acqu√©rir une comp√©tence absolument cruciale.

**üöÄ Next Level :** Dans le module 2, nous allons transformer vos beaux tokens en vecteurs math√©matiques. C'est l√† que la vraie magie commence !

---

*Derni√®re mise √† jour : [Date] | Version 1.0*
*Contributeurs : [Votre nom] | Retours : [email/discord]*